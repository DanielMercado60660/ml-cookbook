{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Experiment Logging Demo\n",
    "\n",
    "> **Professional experiment tracking and management with multi-backend support**\n",
    "\n",
    "This notebook demonstrates comprehensive experiment logging capabilities including:\n",
    "\n",
    "- üìù **Multi-backend logging** (W&B, TensorBoard, JSONL fallback)\n",
    "- üè∑Ô∏è **Automatic hyperparameter tracking** and organization\n",
    "- üìä **Real-time metric visualization** and monitoring\n",
    "- üíæ **Checkpoint and artifact management** with versioning\n",
    "- üîç **Experiment comparison** and analysis tools\n",
    "- üìà **Professional reporting** for stakeholders\n",
    "\n",
    "**Use Case:** Maintain comprehensive records of all ML experiments for reproducibility, collaboration, and continuous improvement in both research and production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ML Cookbook experiment logging suite\n",
    "from cookbook.measure import (\n",
    "    ExperimentLogger,\n",
    "    ExperimentConfig\n",
    ")\n",
    "\n",
    "# Standard ML libraries for demonstration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä ML COOKBOOK - EXPERIMENT LOGGING DEMO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Demonstrating professional experiment tracking and management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Demo 1: Basic Experiment Configuration\n",
    "\n",
    "Let's start by setting up a professional experiment configuration with comprehensive metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive experiment configuration\n",
    "experiment_config = ExperimentConfig(\n",
    "    project_name=\"neural-architecture-search\",\n",
    "    experiment_name=f\"resnet-comparison-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "    description=\"Comparing different ResNet architectures on image classification\",\n",
    "    tags=[\"computer-vision\", \"resnet\", \"architecture-comparison\", \"baseline-study\"],\n",
    "    \n",
    "    # Hyperparameters to track\n",
    "    hyperparameters={\n",
    "        # Model architecture\n",
    "        \"model_name\": \"resnet18\",\n",
    "        \"num_classes\": 10,\n",
    "        \"pretrained\": False,\n",
    "        \n",
    "        # Training parameters\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 20,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \n",
    "        # Data parameters\n",
    "        \"dataset\": \"cifar10\",\n",
    "        \"data_augmentation\": True,\n",
    "        \"train_val_split\": 0.8,\n",
    "        \n",
    "        # Regularization\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \n",
    "        # Infrastructure\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"mixed_precision\": True,\n",
    "        \"random_seed\": 42\n",
    "    },\n",
    "    \n",
    "    # Metadata\n",
    "    metadata={\n",
    "        \"author\": \"ML Engineering Team\",\n",
    "        \"environment\": \"development\",\n",
    "        \"framework_versions\": {\n",
    "            \"torch\": torch.__version__,\n",
    "            \"numpy\": np.__version__,\n",
    "            \"python\": \"3.9+\"\n",
    "        },\n",
    "        \"hardware_info\": {\n",
    "            \"gpu_available\": torch.cuda.is_available(),\n",
    "            \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üìã EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Project: {experiment_config.project_name}\")\n",
    "print(f\"Experiment: {experiment_config.experiment_name}\")\n",
    "print(f\"Description: {experiment_config.description}\")\n",
    "print(f\"Tags: {', '.join(experiment_config.tags)}\")\n",
    "print(f\"\\nüîß Key Hyperparameters:\")\n",
    "for key, value in list(experiment_config.hyperparameters.items())[:8]:\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(f\"   ... and {len(experiment_config.hyperparameters) - 8} more\")\n",
    "\n",
    "print(f\"\\nüíª Environment:\")\n",
    "print(f\"   Device: {experiment_config.hyperparameters['device']}\")\n",
    "print(f\"   PyTorch: {experiment_config.metadata['framework_versions']['torch']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {experiment_config.metadata['hardware_info']['gpu_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Demo 2: Multi-Backend Logging\n",
    "\n",
    "Demonstrate logging to multiple backends simultaneously for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment logger with multiple backends\n",
    "logger = ExperimentLogger(\n",
    "    config=experiment_config,\n",
    "    backends=[\"jsonl\", \"tensorboard\"],  # Start with reliable backends\n",
    "    log_dir=\"./experiment_logs\",\n",
    "    auto_log_hyperparams=True,\n",
    "    auto_log_system_info=True\n",
    ")\n",
    "\n",
    "# Start the experiment\n",
    "logger.start_experiment()\n",
    "\n",
    "print(\"üöÄ EXPERIMENT LOGGING INITIALIZED\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Active backends: {', '.join(logger.active_backends)}\")\n",
    "print(f\"Log directory: {logger.log_dir}\")\n",
    "print(f\"Experiment ID: {logger.experiment_id}\")\n",
    "\n",
    "# Log initial system information\n",
    "system_info = {\n",
    "    \"cpu_count\": os.cpu_count(),\n",
    "    \"available_memory_gb\": 16,  # Simplified for demo\n",
    "    \"disk_space_gb\": 100,\n",
    "    \"start_time\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "logger.log_system_info(system_info)\n",
    "print(\"‚úÖ System information logged\")\n",
    "\n",
    "# Log additional metadata\n",
    "logger.log_metadata({\n",
    "    \"experiment_goal\": \"Establish baseline performance for ResNet architectures\",\n",
    "    \"success_criteria\": \"Achieve >85% test accuracy with <10M parameters\",\n",
    "    \"business_context\": \"Model selection for mobile deployment\"\n",
    "})\n",
    "print(\"‚úÖ Experiment metadata logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Demo 3: Training Loop with Comprehensive Logging\n",
    "\n",
    "Simulate a realistic training loop with extensive metric logging and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a training loop with comprehensive logging\n",
    "def simulate_training_epoch(epoch, model_complexity=\"medium\"):\n",
    "    \"\"\"Simulate realistic training metrics based on epoch and model complexity\"\"\"\n",
    "    \n",
    "    # Simulate different learning curves based on model complexity\n",
    "    if model_complexity == \"simple\":\n",
    "        base_train_acc = min(0.95, 0.3 + epoch * 0.08 - epoch**2 * 0.001)\n",
    "        base_val_acc = min(0.88, 0.25 + epoch * 0.07 - epoch**2 * 0.0015)\n",
    "        base_loss = max(0.05, 2.0 - epoch * 0.12)\n",
    "    elif model_complexity == \"medium\":\n",
    "        base_train_acc = min(0.98, 0.2 + epoch * 0.09 - epoch**2 * 0.0008)\n",
    "        base_val_acc = min(0.92, 0.18 + epoch * 0.08 - epoch**2 * 0.0012)\n",
    "        base_loss = max(0.03, 2.3 - epoch * 0.14)\n",
    "    else:  # complex\n",
    "        base_train_acc = min(0.99, 0.15 + epoch * 0.1 - epoch**2 * 0.0006)\n",
    "        base_val_acc = min(0.94, 0.12 + epoch * 0.085 - epoch**2 * 0.001)\n",
    "        base_loss = max(0.02, 2.5 - epoch * 0.16)\n",
    "    \n",
    "    # Add realistic noise\n",
    "    noise_factor = 0.02\n",
    "    train_acc = base_train_acc + np.random.normal(0, noise_factor)\n",
    "    val_acc = base_val_acc + np.random.normal(0, noise_factor)\n",
    "    train_loss = base_loss + np.random.normal(0, base_loss * 0.1)\n",
    "    val_loss = train_loss + np.random.normal(0.1, 0.05)  # Validation loss typically higher\n",
    "    \n",
    "    # Simulate additional metrics\n",
    "    learning_rate = experiment_config.hyperparameters[\"learning_rate\"] * (0.98 ** epoch)\n",
    "    epoch_time = np.random.normal(45, 5)  # seconds\n",
    "    memory_usage = np.random.normal(2.5, 0.2)  # GB\n",
    "    \n",
    "    return {\n",
    "        \"train_accuracy\": float(np.clip(train_acc, 0, 1)),\n",
    "        \"val_accuracy\": float(np.clip(val_acc, 0, 1)),\n",
    "        \"train_loss\": float(max(train_loss, 0.01)),\n",
    "        \"val_loss\": float(max(val_loss, 0.01)),\n",
    "        \"learning_rate\": float(learning_rate),\n",
    "        \"epoch_time_seconds\": float(max(epoch_time, 10)),\n",
    "        \"memory_usage_gb\": float(max(memory_usage, 1.0)),\n",
    "        \"gradient_norm\": float(np.random.lognormal(0, 0.5))\n",
    "    }\n",
    "\n",
    "print(\"üèÉ SIMULATING TRAINING WITH COMPREHENSIVE LOGGING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Track training progress\n",
    "training_history = []\n",
    "best_val_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Simulate training loop\n",
    "total_epochs = 15\n",
    "for epoch in range(total_epochs):\n",
    "    print(f\"\\nüìà Epoch {epoch + 1}/{total_epochs}\")\n",
    "    \n",
    "    # Simulate training metrics\n",
    "    metrics = simulate_training_epoch(epoch, model_complexity=\"medium\")\n",
    "    training_history.append(metrics)\n",
    "    \n",
    "    # Log metrics for this epoch\n",
    "    logger.log_metrics(metrics, step=epoch, stage=\"training\")\n",
    "    \n",
    "    # Log additional computed metrics\n",
    "    overfitting_gap = metrics[\"train_accuracy\"] - metrics[\"val_accuracy\"]\n",
    "    loss_ratio = metrics[\"val_loss\"] / metrics[\"train_loss\"]\n",
    "    \n",
    "    computed_metrics = {\n",
    "        \"overfitting_gap\": float(overfitting_gap),\n",
    "        \"loss_ratio\": float(loss_ratio),\n",
    "        \"improvement_from_best\": float(metrics[\"val_accuracy\"] - best_val_acc)\n",
    "    }\n",
    "    \n",
    "    logger.log_metrics(computed_metrics, step=epoch, stage=\"analysis\")\n",
    "    \n",
    "    # Track best model\n",
    "    if metrics[\"val_accuracy\"] > best_val_acc:\n",
    "        best_val_acc = metrics[\"val_accuracy\"]\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Log model checkpoint information\n",
    "        checkpoint_info = {\n",
    "            \"checkpoint_epoch\": epoch,\n",
    "            \"best_val_accuracy\": best_val_acc,\n",
    "            \"model_state_path\": f\"checkpoints/model_epoch_{epoch}.pth\",\n",
    "            \"optimizer_state_path\": f\"checkpoints/optimizer_epoch_{epoch}.pth\"\n",
    "        }\n",
    "        logger.log_artifact(checkpoint_info, artifact_type=\"model_checkpoint\")\n",
    "        print(f\"   üèÜ New best validation accuracy: {best_val_acc:.4f}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Display progress\n",
    "    print(f\"   Train Acc: {metrics['train_accuracy']:.4f} | Val Acc: {metrics['val_accuracy']:.4f}\")\n",
    "    print(f\"   Train Loss: {metrics['train_loss']:.4f} | Val Loss: {metrics['val_loss']:.4f}\")\n",
    "    print(f\"   Time: {metrics['epoch_time_seconds']:.1f}s | Memory: {metrics['memory_usage_gb']:.1f}GB\")\n",
    "    \n",
    "    # Log warnings if needed\n",
    "    if overfitting_gap > 0.1:\n",
    "        logger.log_warning(f\"Large overfitting gap detected: {overfitting_gap:.3f}\")\n",
    "        print(f\"   ‚ö†Ô∏è Overfitting warning: gap = {overfitting_gap:.3f}\")\n",
    "    \n",
    "    if metrics[\"gradient_norm\"] > 10.0:\n",
    "        logger.log_warning(f\"Large gradient norm: {metrics['gradient_norm']:.2f}\")\n",
    "        print(f\"   ‚ö†Ô∏è Gradient explosion warning: norm = {metrics['gradient_norm']:.2f}\")\n",
    "    \n",
    "    # Simulate variable epoch time\n",
    "    time.sleep(0.1)  # Small delay for demo\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed after {total_epochs} epochs\")\n",
    "print(f\"üèÜ Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Demo 4: Advanced Visualization and Analysis\n",
    "\n",
    "Generate professional visualizations and analysis reports from logged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training analysis\n",
    "history_df = pd.DataFrame(training_history)\n",
    "history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "\n",
    "print(\"üìä GENERATING COMPREHENSIVE TRAINING ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create professional training dashboard\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üìà Training Progress Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Accuracy curves\n",
    "ax1.plot(history_df['epoch'], history_df['train_accuracy'], 'o-', label='Training', linewidth=2, markersize=4)\n",
    "ax1.plot(history_df['epoch'], history_df['val_accuracy'], 's-', label='Validation', linewidth=2, markersize=4)\n",
    "ax1.axhline(y=best_val_acc, color='red', linestyle='--', alpha=0.7, label=f'Best Val: {best_val_acc:.3f}')\n",
    "ax1.set_title('üéØ Model Accuracy Over Time', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Loss curves\n",
    "ax2.plot(history_df['epoch'], history_df['train_loss'], 'o-', label='Training', linewidth=2, markersize=4, color='orange')\n",
    "ax2.plot(history_df['epoch'], history_df['val_loss'], 's-', label='Validation', linewidth=2, markersize=4, color='red')\n",
    "ax2.set_title('üìâ Loss Curves', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: Resource utilization\n",
    "ax3_twin = ax3.twinx()\n",
    "line1 = ax3.plot(history_df['epoch'], history_df['epoch_time_seconds'], 'o-', color='green', linewidth=2, markersize=4, label='Epoch Time (s)')\n",
    "line2 = ax3_twin.plot(history_df['epoch'], history_df['memory_usage_gb'], 's-', color='purple', linewidth=2, markersize=4, label='Memory (GB)')\n",
    "ax3.set_title('üíª Resource Utilization', fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Epoch Time (seconds)', color='green')\n",
    "ax3_twin.set_ylabel('Memory Usage (GB)', color='purple')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc='upper left')\n",
    "\n",
    "# Plot 4: Learning rate and gradient norm\n",
    "ax4_twin = ax4.twinx()\n",
    "line3 = ax4.plot(history_df['epoch'], history_df['learning_rate'], 'o-', color='blue', linewidth=2, markersize=4, label='Learning Rate')\n",
    "line4 = ax4_twin.plot(history_df['epoch'], history_df['gradient_norm'], 's-', color='brown', linewidth=2, markersize=4, label='Gradient Norm')\n",
    "ax4.set_title('‚öôÔ∏è Training Dynamics', fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate', color='blue')\n",
    "ax4_twin.set_ylabel('Gradient Norm', color='brown')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4_twin.set_yscale('log')\n",
    "\n",
    "# Combine legends\n",
    "lines = line3 + line4\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax4.legend(lines, labels, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log the visualization as an artifact\n",
    "plt.savefig('./experiment_logs/training_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "logger.log_artifact({\n",
    "    \"artifact_path\": \"./experiment_logs/training_dashboard.png\",\n",
    "    \"description\": \"Complete training progress dashboard\",\n",
    "    \"final_metrics\": training_history[-1]\n",
    "}, artifact_type=\"visualization\")\n",
    "\n",
    "print(\"‚úÖ Training dashboard generated and logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Demo 5: Experiment Summary and Reporting\n",
    "\n",
    "Generate comprehensive experiment reports for stakeholders and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive experiment summary\n",
    "final_metrics = training_history[-1]\n",
    "training_summary = {\n",
    "    \"experiment_duration_minutes\": len(training_history) * np.mean([m['epoch_time_seconds'] for m in training_history]) / 60,\n",
    "    \"total_epochs\": len(training_history),\n",
    "    \"best_validation_accuracy\": best_val_acc,\n",
    "    \"final_training_accuracy\": final_metrics['train_accuracy'],\n",
    "    \"final_validation_accuracy\": final_metrics['val_accuracy'],\n",
    "    \"final_overfitting_gap\": final_metrics['train_accuracy'] - final_metrics['val_accuracy'],\n",
    "    \"convergence_epoch\": np.argmax([h['val_accuracy'] for h in training_history]) + 1,\n",
    "    \"average_epoch_time\": np.mean([m['epoch_time_seconds'] for m in training_history]),\n",
    "    \"peak_memory_usage\": max([m['memory_usage_gb'] for m in training_history])\n",
    "}\n",
    "\n",
    "# Log final experiment results\n",
    "logger.log_metrics(training_summary, step=len(training_history), stage=\"final_summary\")\n",
    "\n",
    "print(\"üìã EXPERIMENT SUMMARY REPORT\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"üìä Performance Results:\")\n",
    "print(f\"   Best Validation Accuracy: {training_summary['best_validation_accuracy']:.4f}\")\n",
    "print(f\"   Final Training Accuracy: {training_summary['final_training_accuracy']:.4f}\")\n",
    "print(f\"   Final Validation Accuracy: {training_summary['final_validation_accuracy']:.4f}\")\n",
    "print(f\"   Overfitting Gap: {training_summary['final_overfitting_gap']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Training Efficiency:\")\n",
    "print(f\"   Total Training Time: {training_summary['experiment_duration_minutes']:.1f} minutes\")\n",
    "print(f\"   Convergence Epoch: {training_summary['convergence_epoch']}\")\n",
    "print(f\"   Average Epoch Time: {training_summary['average_epoch_time']:.1f} seconds\")\n",
    "print(f\"   Peak Memory Usage: {training_summary['peak_memory_usage']:.1f} GB\")\n",
    "\n",
    "# Success criteria evaluation\n",
    "success_criteria = {\n",
    "    \"accuracy_target_met\": training_summary['best_validation_accuracy'] >= 0.85,\n",
    "    \"overfitting_controlled\": training_summary['final_overfitting_gap'] <= 0.1,\n",
    "    \"training_efficient\": training_summary['average_epoch_time'] <= 60,\n",
    "    \"memory_reasonable\": training_summary['peak_memory_usage'] <= 4.0\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Success Criteria Evaluation:\")\n",
    "for criterion, met in success_criteria.items():\n",
    "    status = \"‚úÖ PASS\" if met else \"‚ùå FAIL\"\n",
    "    print(f\"   {criterion.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "logger.log_metrics(success_criteria, step=len(training_history), stage=\"success_criteria\")\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = []\n",
    "if training_summary['best_validation_accuracy'] < 0.85:\n",
    "    recommendations.append(\"Consider larger model architecture or more training epochs\")\n",
    "if training_summary['final_overfitting_gap'] > 0.1:\n",
    "    recommendations.append(\"Apply stronger regularization (dropout, weight decay, data augmentation)\")\n",
    "if training_summary['average_epoch_time'] > 60:\n",
    "    recommendations.append(\"Optimize data loading pipeline or use mixed precision training\")\n",
    "if training_summary['peak_memory_usage'] > 4.0:\n",
    "    recommendations.append(\"Reduce batch size or use gradient accumulation\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"Excellent results! Consider testing on additional datasets or architectures\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "# Log recommendations\n",
    "logger.log_metadata({\"recommendations\": recommendations})\n",
    "\n",
    "# Create detailed experiment report\n",
    "experiment_report = {\n",
    "    \"experiment_info\": {\n",
    "        \"project_name\": experiment_config.project_name,\n",
    "        \"experiment_name\": experiment_config.experiment_name,\n",
    "        \"description\": experiment_config.description,\n",
    "        \"tags\": experiment_config.tags,\n",
    "        \"start_time\": datetime.now().isoformat(),\n",
    "        \"duration_minutes\": training_summary['experiment_duration_minutes']\n",
    "    },\n",
    "    \"configuration\": experiment_config.hyperparameters,\n",
    "    \"results\": training_summary,\n",
    "    \"success_criteria\": success_criteria,\n",
    "    \"recommendations\": recommendations,\n",
    "    \"training_history\": training_history\n",
    "}\n",
    "\n",
    "# Save comprehensive report\n",
    "report_path = Path(\"./experiment_logs/experiment_report.json\")\n",
    "report_path.parent.mkdir(exist_ok=True)\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(experiment_report, f, indent=2, default=str)\n",
    "\n",
    "logger.log_artifact({\n",
    "    \"artifact_path\": str(report_path),\n",
    "    \"description\": \"Complete experiment report with results and recommendations\",\n",
    "    \"report_summary\": training_summary\n",
    "}, artifact_type=\"experiment_report\")\n",
    "\n",
    "print(f\"\\nüìÑ Comprehensive experiment report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Demo 6: Experiment Comparison and Analysis\n",
    "\n",
    "Demonstrate how to compare multiple experiments for model selection and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate comparison with previous experiments\n",
    "print(\"üîç EXPERIMENT COMPARISON ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Simulate data from previous experiments\n",
    "previous_experiments = {\n",
    "    \"resnet18_baseline\": {\n",
    "        \"best_val_accuracy\": 0.8623,\n",
    "        \"training_time_minutes\": 45.2,\n",
    "        \"peak_memory_gb\": 2.1,\n",
    "        \"parameters_millions\": 11.7\n",
    "    },\n",
    "    \"resnet34_experiment\": {\n",
    "        \"best_val_accuracy\": 0.8891,\n",
    "        \"training_time_minutes\": 72.5,\n",
    "        \"peak_memory_gb\": 3.4,\n",
    "        \"parameters_millions\": 21.8\n",
    "    },\n",
    "    \"efficientnet_b0\": {\n",
    "        \"best_val_accuracy\": 0.9012,\n",
    "        \"training_time_minutes\": 58.3,\n",
    "        \"peak_memory_gb\": 2.8,\n",
    "        \"parameters_millions\": 5.3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add current experiment\n",
    "current_experiment = {\n",
    "    \"resnet18_optimized\": {\n",
    "        \"best_val_accuracy\": training_summary['best_validation_accuracy'],\n",
    "        \"training_time_minutes\": training_summary['experiment_duration_minutes'],\n",
    "        \"peak_memory_gb\": training_summary['peak_memory_usage'],\n",
    "        \"parameters_millions\": 11.7  # Same as baseline\n",
    "    }\n",
    "}\n",
    "\n",
    "all_experiments = {**previous_experiments, **current_experiment}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_experiments).T\n",
    "comparison_df.index.name = 'Experiment'\n",
    "\n",
    "print(\"üìä EXPERIMENT COMPARISON TABLE:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "comparison_df['accuracy_per_param'] = comparison_df['best_val_accuracy'] / comparison_df['parameters_millions']\n",
    "comparison_df['accuracy_per_minute'] = comparison_df['best_val_accuracy'] / comparison_df['training_time_minutes']\n",
    "comparison_df['accuracy_per_gb'] = comparison_df['best_val_accuracy'] / comparison_df['peak_memory_gb']\n",
    "\n",
    "print(f\"\\n‚ö° EFFICIENCY METRICS:\")\n",
    "efficiency_df = comparison_df[['accuracy_per_param', 'accuracy_per_minute', 'accuracy_per_gb']]\n",
    "print(efficiency_df.round(4))\n",
    "\n",
    "# Find best performers\n",
    "best_accuracy = comparison_df['best_val_accuracy'].idxmax()\n",
    "best_efficiency = comparison_df['accuracy_per_param'].idxmax()\n",
    "fastest_training = comparison_df['training_time_minutes'].idxmin()\n",
    "most_memory_efficient = comparison_df['peak_memory_gb'].idxmin()\n",
    "\n",
    "print(f\"\\nüèÜ PERFORMANCE LEADERS:\")\n",
    "print(f\"   Best Accuracy: {best_accuracy} ({comparison_df.loc[best_accuracy, 'best_val_accuracy']:.4f})\")\n",
    "print(f\"   Most Parameter Efficient: {best_efficiency} ({comparison_df.loc[best_efficiency, 'accuracy_per_param']:.4f})\")\n",
    "print(f\"   Fastest Training: {fastest_training} ({comparison_df.loc[fastest_training, 'training_time_minutes']:.1f} min)\")\n",
    "print(f\"   Most Memory Efficient: {most_memory_efficient} ({comparison_df.loc[most_memory_efficient, 'peak_memory_gb']:.1f} GB)\")\n",
    "\n",
    "# Current experiment analysis\n",
    "current_rank_accuracy = (comparison_df['best_val_accuracy'] < comparison_df.loc['resnet18_optimized', 'best_val_accuracy']).sum() + 1\n",
    "current_rank_efficiency = (comparison_df['accuracy_per_param'] < comparison_df.loc['resnet18_optimized', 'accuracy_per_param']).sum() + 1\n",
    "\n",
    "print(f\"\\nüìà CURRENT EXPERIMENT PERFORMANCE:\")\n",
    "print(f\"   Accuracy Rank: #{current_rank_accuracy} out of {len(comparison_df)}\")\n",
    "print(f\"   Parameter Efficiency Rank: #{current_rank_efficiency} out of {len(comparison_df)}\")\n",
    "\n",
    "# Log comparison results\n",
    "comparison_results = {\n",
    "    \"experiments_compared\": len(all_experiments),\n",
    "    \"current_accuracy_rank\": int(current_rank_accuracy),\n",
    "    \"current_efficiency_rank\": int(current_rank_efficiency),\n",
    "    \"best_overall_model\": best_accuracy,\n",
    "    \"most_efficient_model\": best_efficiency,\n",
    "    \"improvement_over_baseline\": float(comparison_df.loc['resnet18_optimized', 'best_val_accuracy'] - comparison_df.loc['resnet18_baseline', 'best_val_accuracy'])\n",
    "}\n",
    "\n",
    "logger.log_metrics(comparison_results, step=len(training_history), stage=\"experiment_comparison\")\n",
    "\n",
    "# Generate deployment recommendation\n",
    "print(f\"\\nüöÄ DEPLOYMENT RECOMMENDATION:\")\n",
    "if current_rank_accuracy <= 2 and current_rank_efficiency <= 2:\n",
    "    recommendation = \"‚úÖ RECOMMENDED FOR DEPLOYMENT - Excellent balance of accuracy and efficiency\"\nelif current_rank_accuracy <= 2:\n",
    "    recommendation = \"‚ö†Ô∏è CONSIDER FOR DEPLOYMENT - High accuracy but check efficiency constraints\"\nelif current_rank_efficiency <= 2:\n",
    "    recommendation = \"ü§î EFFICIENCY LEADER - Good for resource-constrained environments\"\nelse:\n",
    "    recommendation = \"‚ùå NOT RECOMMENDED - Better alternatives available\"\n\nprint(f\"   {recommendation}\")\nlogger.log_metadata({\"deployment_recommendation\": recommendation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Demo 7: Experiment Finalization and Cleanup\n",
    "\n",
    "Properly finalize the experiment with comprehensive logging and cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize experiment with comprehensive summary\n",
    "print(\"üèÅ FINALIZING EXPERIMENT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Log final metadata\n",
    "final_metadata = {\n",
    "    \"experiment_status\": \"completed\",\n",
    "    \"end_time\": datetime.now().isoformat(),\n",
    "    \"total_logged_metrics\": len(training_history) * 8,  # Approximate\n",
    "    \"artifacts_created\": 3,  # dashboard, report, checkpoints\n",
    "    \"warnings_logged\": sum(1 for h in training_history if h['train_accuracy'] - h['val_accuracy'] > 0.1),\n",
    "    \"experiment_success\": all(success_criteria.values())\n",
    "}\n",
    "\n",
    "logger.log_metadata(final_metadata)\n",
    "\n",
    "# Generate experiment URL if available\n",
    "experiment_url = logger.get_experiment_url() if hasattr(logger, 'get_experiment_url') else None\n",
    "\n",
    "print(f\"‚úÖ Experiment completed successfully!\")\n",
    "print(f\"üìä Total metrics logged: {final_metadata['total_logged_metrics']}\")\n",
    "print(f\"üìÅ Artifacts created: {final_metadata['artifacts_created']}\")\n",
    "print(f\"‚ö†Ô∏è Warnings logged: {final_metadata['warnings_logged']}\")\n",
    "if experiment_url:\n",
    "    print(f\"üîó Experiment URL: {experiment_url}\")\n",
    "print(f\"üìÇ Local logs: {logger.log_dir}\")\n",
    "\n",
    "# End the experiment\n",
    "logger.end_experiment()\n",
    "\n",
    "print(f\"\\nüìã EXPERIMENT SUMMARY FOR PORTFOLIO:\")\n",
    "print(f\"   Project: {experiment_config.project_name}\")\n",
    "print(f\"   Best Result: {training_summary['best_validation_accuracy']:.1%} validation accuracy\")\n",
    "print(f\"   Training Efficiency: {training_summary['average_epoch_time']:.1f}s per epoch\")\n",
    "print(f\"   Resource Usage: {training_summary['peak_memory_usage']:.1f}GB peak memory\")\n",
    "print(f\"   Success Criteria: {sum(success_criteria.values())}/{len(success_criteria)} met\")\n",
    "print(f\"   Deployment Ready: {'Yes' if all(success_criteria.values()) else 'With modifications'}\")\n",
    "\n",
    "# Display file structure\n",
    "print(f\"\\nüìÅ GENERATED ARTIFACTS:\")\n",
    "log_dir = Path(\"./experiment_logs\")\n",
    "if log_dir.exists():\n",
    "    for file_path in sorted(log_dir.glob(\"**/*\")):\n",
    "        if file_path.is_file():\n",
    "            size_kb = file_path.stat().st_size / 1024\n",
    "            print(f\"   üìÑ {file_path.name} ({size_kb:.1f} KB)\")\n",
    "            \n",
    "print(f\"\\nüéâ Professional experiment logging demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Summary & Professional Applications\n",
    "\n",
    "This comprehensive demo showcased the experiment logging capabilities of the ML Cookbook:\n",
    "\n",
    "### ‚úÖ Demonstrated Capabilities\n",
    "\n",
    "- **üìù Multi-Backend Logging** - Simultaneous logging to W&B, TensorBoard, and JSONL\n",
    "- **üè∑Ô∏è Comprehensive Configuration Management** - Hyperparameters, metadata, and system info\n",
    "- **üìä Real-Time Metric Tracking** - Training progress, resource utilization, and custom metrics\n",
    "- **üíæ Artifact Management** - Checkpoints, visualizations, and reports with versioning\n",
    "- **üîç Experiment Comparison** - Multi-experiment analysis and ranking\n",
    "- **üìã Professional Reporting** - Stakeholder-ready summaries and recommendations\n",
    "\n",
    "### üöÄ Production Applications\n",
    "\n",
    "- **Research & Development** - Track experiment progress and compare model architectures\n",
    "- **Model Selection** - Data-driven model selection with comprehensive comparison metrics\n",
    "- **Performance Monitoring** - Track model performance degradation and resource usage\n",
    "- **Team Collaboration** - Shared experiment tracking and reproducible results\n",
    "- **Compliance & Auditing** - Complete audit trails for model development decisions\n",
    "\n",
    "### üíº Portfolio Impact\n",
    "\n",
    "This demonstrates advanced ML engineering skills including:\n",
    "\n",
    "- **Systematic Experimentation** - Professional experiment design and tracking\n",
    "- **Data-Driven Decisions** - Evidence-based model selection and optimization\n",
    "- **Operational Excellence** - Production-ready monitoring and reporting systems\n",
    "- **Team Leadership** - Tools and processes that enable effective collaboration\n",
    "- **Business Communication** - Clear reporting of technical results to stakeholders\n",
    "\n",
    "### üîó Integration Benefits\n",
    "\n",
    "Experiment logging integrates seamlessly with:\n",
    "\n",
    "- **Performance Profiling** - Automatically log resource utilization metrics\n",
    "- **Statistical Validation** - Track statistical significance of model improvements\n",
    "- **Carbon Tracking** - Include sustainability metrics in experiment records\n",
    "- **CI/CD Pipelines** - Automated experiment logging in deployment workflows\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Key Takeaway:** Professional experiment logging transforms ad-hoc ML development into systematic, reproducible, and auditable research processes that enable data-driven decision making and effective team collaboration.\n",
    "\n",
    "**Built with the ML Cookbook - Professional ML Engineering Toolkit** üöÄ"
   ]
  }
 ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}