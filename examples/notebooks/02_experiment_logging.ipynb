{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Experiment Logging Demo\n",
    "\n",
    "> **Professional experiment tracking and management with multi-backend support**\n",
    "\n",
    "This notebook demonstrates comprehensive experiment logging capabilities including:\n",
    "\n",
    "- 📝 **Multi-backend logging** (W&B, TensorBoard, JSONL fallback)\n",
    "- 🏷️ **Automatic hyperparameter tracking** and organization\n",
    "- 📊 **Real-time metric visualization** and monitoring\n",
    "- 💾 **Checkpoint and artifact management** with versioning\n",
    "- 🔍 **Experiment comparison** and analysis tools\n",
    "- 📈 **Professional reporting** for stakeholders\n",
    "\n",
    "**Use Case:** Maintain comprehensive records of all ML experiments for reproducibility, collaboration, and continuous improvement in both research and production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ML Cookbook experiment logging suite\n",
    "from cookbook.measure import (\n",
    "    ExperimentLogger,\n",
    "    ExperimentConfig\n",
    ")\n",
    "\n",
    "# Standard ML libraries for demonstration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"📊 ML COOKBOOK - EXPERIMENT LOGGING DEMO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Demonstrating professional experiment tracking and management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Demo 1: Basic Experiment Configuration\n",
    "\n",
    "Let's start by setting up a professional experiment configuration with comprehensive metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive experiment configuration\n",
    "experiment_config = ExperimentConfig(\n",
    "    project_name=\"neural-architecture-search\",\n",
    "    experiment_name=f\"resnet-comparison-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "    description=\"Comparing different ResNet architectures on image classification\",\n",
    "    tags=[\"computer-vision\", \"resnet\", \"architecture-comparison\", \"baseline-study\"],\n",
    "    \n",
    "    # Hyperparameters to track\n",
    "    hyperparameters={\n",
    "        # Model architecture\n",
    "        \"model_name\": \"resnet18\",\n",
    "        \"num_classes\": 10,\n",
    "        \"pretrained\": False,\n",
    "        \n",
    "        # Training parameters\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 20,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \n",
    "        # Data parameters\n",
    "        \"dataset\": \"cifar10\",\n",
    "        \"data_augmentation\": True,\n",
    "        \"train_val_split\": 0.8,\n",
    "        \n",
    "        # Regularization\n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"early_stopping_patience\": 5,\n",
    "        \n",
    "        # Infrastructure\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"mixed_precision\": True,\n",
    "        \"random_seed\": 42\n",
    "    },\n",
    "    \n",
    "    # Metadata\n",
    "    metadata={\n",
    "        \"author\": \"ML Engineering Team\",\n",
    "        \"environment\": \"development\",\n",
    "        \"framework_versions\": {\n",
    "            \"torch\": torch.__version__,\n",
    "            \"numpy\": np.__version__,\n",
    "            \"python\": \"3.9+\"\n",
    "        },\n",
    "        \"hardware_info\": {\n",
    "            \"gpu_available\": torch.cuda.is_available(),\n",
    "            \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"📋 EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Project: {experiment_config.project_name}\")\n",
    "print(f\"Experiment: {experiment_config.experiment_name}\")\n",
    "print(f\"Description: {experiment_config.description}\")\n",
    "print(f\"Tags: {', '.join(experiment_config.tags)}\")\n",
    "print(f\"\\n🔧 Key Hyperparameters:\")\n",
    "for key, value in list(experiment_config.hyperparameters.items())[:8]:\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(f\"   ... and {len(experiment_config.hyperparameters) - 8} more\")\n",
    "\n",
    "print(f\"\\n💻 Environment:\")\n",
    "print(f\"   Device: {experiment_config.hyperparameters['device']}\")\n",
    "print(f\"   PyTorch: {experiment_config.metadata['framework_versions']['torch']}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {experiment_config.metadata['hardware_info']['gpu_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Demo 2: Multi-Backend Logging\n",
    "\n",
    "Demonstrate logging to multiple backends simultaneously for maximum flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment logger with multiple backends\n",
    "logger = ExperimentLogger(\n",
    "    config=experiment_config,\n",
    "    backends=[\"jsonl\", \"tensorboard\"],  # Start with reliable backends\n",
    "    log_dir=\"./experiment_logs\",\n",
    "    auto_log_hyperparams=True,\n",
    "    auto_log_system_info=True\n",
    ")\n",
    "\n",
    "# Start the experiment\n",
    "logger.start_experiment()\n",
    "\n",
    "print(\"🚀 EXPERIMENT LOGGING INITIALIZED\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Active backends: {', '.join(logger.active_backends)}\")\n",
    "print(f\"Log directory: {logger.log_dir}\")\n",
    "print(f\"Experiment ID: {logger.experiment_id}\")\n",
    "\n",
    "# Log initial system information\n",
    "system_info = {\n",
    "    \"cpu_count\": os.cpu_count(),\n",
    "    \"available_memory_gb\": 16,  # Simplified for demo\n",
    "    \"disk_space_gb\": 100,\n",
    "    \"start_time\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "logger.log_system_info(system_info)\n",
    "print(\"✅ System information logged\")\n",
    "\n",
    "# Log additional metadata\n",
    "logger.log_metadata({\n",
    "    \"experiment_goal\": \"Establish baseline performance for ResNet architectures\",\n",
    "    \"success_criteria\": \"Achieve >85% test accuracy with <10M parameters\",\n",
    "    \"business_context\": \"Model selection for mobile deployment\"\n",
    "})\n",
    "print(\"✅ Experiment metadata logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Demo 3: Training Loop with Comprehensive Logging\n",
    "\n",
    "Simulate a realistic training loop with extensive metric logging and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a training loop with comprehensive logging\n",
    "def simulate_training_epoch(epoch, model_complexity=\"medium\"):\n",
    "    \"\"\"Simulate realistic training metrics based on epoch and model complexity\"\"\"\n",
    "    \n",
    "    # Simulate different learning curves based on model complexity\n",
    "    if model_complexity == \"simple\":\n",
    "        base_train_acc = min(0.95, 0.3 + epoch * 0.08 - epoch**2 * 0.001)\n",
    "        base_val_acc = min(0.88, 0.25 + epoch * 0.07 - epoch**2 * 0.0015)\n",
    "        base_loss = max(0.05, 2.0 - epoch * 0.12)\n",
    "    elif model_complexity == \"medium\":\n",
    "        base_train_acc = min(0.98, 0.2 + epoch * 0.09 - epoch**2 * 0.0008)\n",
    "        base_val_acc = min(0.92, 0.18 + epoch * 0.08 - epoch**2 * 0.0012)\n",
    "        base_loss = max(0.03, 2.3 - epoch * 0.14)\n",
    "    else:  # complex\n",
    "        base_train_acc = min(0.99, 0.15 + epoch * 0.1 - epoch**2 * 0.0006)\n",
    "        base_val_acc = min(0.94, 0.12 + epoch * 0.085 - epoch**2 * 0.001)\n",
    "        base_loss = max(0.02, 2.5 - epoch * 0.16)\n",
    "    \n",
    "    # Add realistic noise\n",
    "    noise_factor = 0.02\n",
    "    train_acc = base_train_acc + np.random.normal(0, noise_factor)\n",
    "    val_acc = base_val_acc + np.random.normal(0, noise_factor)\n",
    "    train_loss = base_loss + np.random.normal(0, base_loss * 0.1)\n",
    "    val_loss = train_loss + np.random.normal(0.1, 0.05)  # Validation loss typically higher\n",
    "    \n",
    "    # Simulate additional metrics\n",
    "    learning_rate = experiment_config.hyperparameters[\"learning_rate\"] * (0.98 ** epoch)\n",
    "    epoch_time = np.random.normal(45, 5)  # seconds\n",
    "    memory_usage = np.random.normal(2.5, 0.2)  # GB\n",
    "    \n",
    "    return {\n",
    "        \"train_accuracy\": float(np.clip(train_acc, 0, 1)),\n",
    "        \"val_accuracy\": float(np.clip(val_acc, 0, 1)),\n",
    "        \"train_loss\": float(max(train_loss, 0.01)),\n",
    "        \"val_loss\": float(max(val_loss, 0.01)),\n",
    "        \"learning_rate\": float(learning_rate),\n",
    "        \"epoch_time_seconds\": float(max(epoch_time, 10)),\n",
    "        \"memory_usage_gb\": float(max(memory_usage, 1.0)),\n",
    "        \"gradient_norm\": float(np.random.lognormal(0, 0.5))\n",
    "    }\n",
    "\n",
    "print(\"🏃 SIMULATING TRAINING WITH COMPREHENSIVE LOGGING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Track training progress\n",
    "training_history = []\n",
    "best_val_acc = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Simulate training loop\n",
    "total_epochs = 15\n",
    "for epoch in range(total_epochs):\n",
    "    print(f\"\\n📈 Epoch {epoch + 1}/{total_epochs}\")\n",
    "    \n",
    "    # Simulate training metrics\n",
    "    metrics = simulate_training_epoch(epoch, model_complexity=\"medium\")\n",
    "    training_history.append(metrics)\n",
    "    \n",
    "    # Log metrics for this epoch\n",
    "    logger.log_metrics(metrics, step=epoch, stage=\"training\")\n",
    "    \n",
    "    # Log additional computed metrics\n",
    "    overfitting_gap = metrics[\"train_accuracy\"] - metrics[\"val_accuracy\"]\n",
    "    loss_ratio = metrics[\"val_loss\"] / metrics[\"train_loss\"]\n",
    "    \n",
    "    computed_metrics = {\n",
    "        \"overfitting_gap\": float(overfitting_gap),\n",
    "        \"loss_ratio\": float(loss_ratio),\n",
    "        \"improvement_from_best\": float(metrics[\"val_accuracy\"] - best_val_acc)\n",
    "    }\n",
    "    \n",
    "    logger.log_metrics(computed_metrics, step=epoch, stage=\"analysis\")\n",
    "    \n",
    "    # Track best model\n",
    "    if metrics[\"val_accuracy\"] > best_val_acc:\n",
    "        best_val_acc = metrics[\"val_accuracy\"]\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Log model checkpoint information\n",
    "        checkpoint_info = {\n",
    "            \"checkpoint_epoch\": epoch,\n",
    "            \"best_val_accuracy\": best_val_acc,\n",
    "            \"model_state_path\": f\"checkpoints/model_epoch_{epoch}.pth\",\n",
    "            \"optimizer_state_path\": f\"checkpoints/optimizer_epoch_{epoch}.pth\"\n",
    "        }\n",
    "        logger.log_artifact(checkpoint_info, artifact_type=\"model_checkpoint\")\n",
    "        print(f\"   🏆 New best validation accuracy: {best_val_acc:.4f}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Display progress\n",
    "    print(f\"   Train Acc: {metrics['train_accuracy']:.4f} | Val Acc: {metrics['val_accuracy']:.4f}\")\n",
    "    print(f\"   Train Loss: {metrics['train_loss']:.4f} | Val Loss: {metrics['val_loss']:.4f}\")\n",
    "    print(f\"   Time: {metrics['epoch_time_seconds']:.1f}s | Memory: {metrics['memory_usage_gb']:.1f}GB\")\n",
    "    \n",
    "    # Log warnings if needed\n",
    "    if overfitting_gap > 0.1:\n",
    "        logger.log_warning(f\"Large overfitting gap detected: {overfitting_gap:.3f}\")\n",
    "        print(f\"   ⚠️ Overfitting warning: gap = {overfitting_gap:.3f}\")\n",
    "    \n",
    "    if metrics[\"gradient_norm\"] > 10.0:\n",
    "        logger.log_warning(f\"Large gradient norm: {metrics['gradient_norm']:.2f}\")\n",
    "        print(f\"   ⚠️ Gradient explosion warning: norm = {metrics['gradient_norm']:.2f}\")\n",
    "    \n",
    "    # Simulate variable epoch time\n",
    "    time.sleep(0.1)  # Small delay for demo\n",
    "\n",
    "print(f\"\\n✅ Training completed after {total_epochs} epochs\")\n",
    "print(f\"🏆 Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Demo 4: Advanced Visualization and Analysis\n",
    "\n",
    "Generate professional visualizations and analysis reports from logged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training analysis\n",
    "history_df = pd.DataFrame(training_history)\n",
    "history_df['epoch'] = range(1, len(history_df) + 1)\n",
    "\n",
    "print(\"📊 GENERATING COMPREHENSIVE TRAINING ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create professional training dashboard\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('📈 Training Progress Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Accuracy curves\n",
    "ax1.plot(history_df['epoch'], history_df['train_accuracy'], 'o-', label='Training', linewidth=2, markersize=4)\n",
    "ax1.plot(history_df['epoch'], history_df['val_accuracy'], 's-', label='Validation', linewidth=2, markersize=4)\n",
    "ax1.axhline(y=best_val_acc, color='red', linestyle='--', alpha=0.7, label=f'Best Val: {best_val_acc:.3f}')\n",
    "ax1.set_title('🎯 Model Accuracy Over Time', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Loss curves\n",
    "ax2.plot(history_df['epoch'], history_df['train_loss'], 'o-', label='Training', linewidth=2, markersize=4, color='orange')\n",
    "ax2.plot(history_df['epoch'], history_df['val_loss'], 's-', label='Validation', linewidth=2, markersize=4, color='red')\n",
    "ax2.set_title('📉 Loss Curves', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: Resource utilization\n",
    "ax3_twin = ax3.twinx()\n",
    "line1 = ax3.plot(history_df['epoch'], history_df['epoch_time_seconds'], 'o-', color='green', linewidth=2, markersize=4, label='Epoch Time (s)')\n",
    "line2 = ax3_twin.plot(history_df['epoch'], history_df['memory_usage_gb'], 's-', color='purple', linewidth=2, markersize=4, label='Memory (GB)')\n",
    "ax3.set_title('💻 Resource Utilization', fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Epoch Time (seconds)', color='green')\n",
    "ax3_twin.set_ylabel('Memory Usage (GB)', color='purple')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax3.legend(lines, labels, loc='upper left')\n",
    "\n",
    "# Plot 4: Learning rate and gradient norm\n",
    "ax4_twin = ax4.twinx()\n",
    "line3 = ax4.plot(history_df['epoch'], history_df['learning_rate'], 'o-', color='blue', linewidth=2, markersize=4, label='Learning Rate')\n",
    "line4 = ax4_twin.plot(history_df['epoch'], history_df['gradient_norm'], 's-', color='brown', linewidth=2, markersize=4, label='Gradient Norm')\n",
    "ax4.set_title('⚙️ Training Dynamics', fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate', color='blue')\n",
    "ax4_twin.set_ylabel('Gradient Norm', color='brown')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4_twin.set_yscale('log')\n",
    "\n",
    "# Combine legends\n",
    "lines = line3 + line4\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax4.legend(lines, labels, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log the visualization as an artifact\n",
    "plt.savefig('./experiment_logs/training_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "logger.log_artifact({\n",
    "    \"artifact_path\": \"./experiment_logs/training_dashboard.png\",\n",
    "    \"description\": \"Complete training progress dashboard\",\n",
    "    \"final_metrics\": training_history[-1]\n",
    "}, artifact_type=\"visualization\")\n",
    "\n",
    "print(\"✅ Training dashboard generated and logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Demo 5: Experiment Summary and Reporting\n",
    "\n",
    "Generate comprehensive experiment reports for stakeholders and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive experiment summary\n",
    "final_metrics = training_history[-1]\n",
    "training_summary = {\n",
    "    \"experiment_duration_minutes\": len(training_history) * np.mean([m['epoch_time_seconds'] for m in training_history]) / 60,\n",
    "    \"total_epochs\": len(training_history),\n",
    "    \"best_validation_accuracy\": best_val_acc,\n",
    "    \"final_training_accuracy\": final_metrics['train_accuracy'],\n",
    "    \"final_validation_accuracy\": final_metrics['val_accuracy'],\n",
    "    \"final_overfitting_gap\": final_metrics['train_accuracy'] - final_metrics['val_accuracy'],\n",
    "    \"convergence_epoch\": np.argmax([h['val_accuracy'] for h in training_history]) + 1,\n",
    "    \"average_epoch_time\": np.mean([m['epoch_time_seconds'] for m in training_history]),\n",
    "    \"peak_memory_usage\": max([m['memory_usage_gb'] for m in training_history])\n",
    "}\n",
    "\n",
    "# Log final experiment results\n",
    "logger.log_metrics(training_summary, step=len(training_history), stage=\"final_summary\")\n",
    "\n",
    "print(\"📋 EXPERIMENT SUMMARY REPORT\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"📊 Performance Results:\")\n",
    "print(f\"   Best Validation Accuracy: {training_summary['best_validation_accuracy']:.4f}\")\n",
    "print(f\"   Final Training Accuracy: {training_summary['final_training_accuracy']:.4f}\")\n",
    "print(f\"   Final Validation Accuracy: {training_summary['final_validation_accuracy']:.4f}\")\n",
    "print(f\"   Overfitting Gap: {training_summary['final_overfitting_gap']:.4f}\")\n",
    "\n",
    "print(f\"\\n⏱️ Training Efficiency:\")\n",
    "print(f\"   Total Training Time: {training_summary['experiment_duration_minutes']:.1f} minutes\")\n",
    "print(f\"   Convergence Epoch: {training_summary['convergence_epoch']}\")\n",
    "print(f\"   Average Epoch Time: {training_summary['average_epoch_time']:.1f} seconds\")\n",
    "print(f\"   Peak Memory Usage: {training_summary['peak_memory_usage']:.1f} GB\")\n",
    "\n",
    "# Success criteria evaluation\n",
    "success_criteria = {\n",
    "    \"accuracy_target_met\": training_summary['best_validation_accuracy'] >= 0.85,\n",
    "    \"overfitting_controlled\": training_summary['final_overfitting_gap'] <= 0.1,\n",
    "    \"training_efficient\": training_summary['average_epoch_time'] <= 60,\n",
    "    \"memory_reasonable\": training_summary['peak_memory_usage'] <= 4.0\n",
    "}\n",
    "\n",
    "print(f\"\\n🎯 Success Criteria Evaluation:\")\n",
    "for criterion, met in success_criteria.items():\n",
    "    status = \"✅ PASS\" if met else \"❌ FAIL\"\n",
    "    print(f\"   {criterion.replace('_', ' ').title()}: {status}\")\n",
    "\n",
    "logger.log_metrics(success_criteria, step=len(training_history), stage=\"success_criteria\")\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = []\n",
    "if training_summary['best_validation_accuracy'] < 0.85:\n",
    "    recommendations.append(\"Consider larger model architecture or more training epochs\")\n",
    "if training_summary['final_overfitting_gap'] > 0.1:\n",
    "    recommendations.append(\"Apply stronger regularization (dropout, weight decay, data augmentation)\")\n",
    "if training_summary['average_epoch_time'] > 60:\n",
    "    recommendations.append(\"Optimize data loading pipeline or use mixed precision training\")\n",
    "if training_summary['peak_memory_usage'] > 4.0:\n",
    "    recommendations.append(\"Reduce batch size or use gradient accumulation\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"Excellent results! Consider testing on additional datasets or architectures\")\n",
    "\n",
    "print(f\"\\n💡 Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "# Log recommendations\n",
    "logger.log_metadata({\"recommendations\": recommendations})\n",
    "\n",
    "# Create detailed experiment report\n",
    "experiment_report = {\n",
    "    \"experiment_info\": {\n",
    "        \"project_name\": experiment_config.project_name,\n",
    "        \"experiment_name\": experiment_config.experiment_name,\n",
    "        \"description\": experiment_config.description,\n",
    "        \"tags\": experiment_config.tags,\n",
    "        \"start_time\": datetime.now().isoformat(),\n",
    "        \"duration_minutes\": training_summary['experiment_duration_minutes']\n",
    "    },\n",
    "    \"configuration\": experiment_config.hyperparameters,\n",
    "    \"results\": training_summary,\n",
    "    \"success_criteria\": success_criteria,\n",
    "    \"recommendations\": recommendations,\n",
    "    \"training_history\": training_history\n",
    "}\n",
    "\n",
    "# Save comprehensive report\n",
    "report_path = Path(\"./experiment_logs/experiment_report.json\")\n",
    "report_path.parent.mkdir(exist_ok=True)\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(experiment_report, f, indent=2, default=str)\n",
    "\n",
    "logger.log_artifact({\n",
    "    \"artifact_path\": str(report_path),\n",
    "    \"description\": \"Complete experiment report with results and recommendations\",\n",
    "    \"report_summary\": training_summary\n",
    "}, artifact_type=\"experiment_report\")\n",
    "\n",
    "print(f\"\\n📄 Comprehensive experiment report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Demo 6: Experiment Comparison and Analysis\n",
    "\n",
    "Demonstrate how to compare multiple experiments for model selection and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate comparison with previous experiments\n",
    "print(\"🔍 EXPERIMENT COMPARISON ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Simulate data from previous experiments\n",
    "previous_experiments = {\n",
    "    \"resnet18_baseline\": {\n",
    "        \"best_val_accuracy\": 0.8623,\n",
    "        \"training_time_minutes\": 45.2,\n",
    "        \"peak_memory_gb\": 2.1,\n",
    "        \"parameters_millions\": 11.7\n",
    "    },\n",
    "    \"resnet34_experiment\": {\n",
    "        \"best_val_accuracy\": 0.8891,\n",
    "        \"training_time_minutes\": 72.5,\n",
    "        \"peak_memory_gb\": 3.4,\n",
    "        \"parameters_millions\": 21.8\n",
    "    },\n",
    "    \"efficientnet_b0\": {\n",
    "        \"best_val_accuracy\": 0.9012,\n",
    "        \"training_time_minutes\": 58.3,\n",
    "        \"peak_memory_gb\": 2.8,\n",
    "        \"parameters_millions\": 5.3\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add current experiment\n",
    "current_experiment = {\n",
    "    \"resnet18_optimized\": {\n",
    "        \"best_val_accuracy\": training_summary['best_validation_accuracy'],\n",
    "        \"training_time_minutes\": training_summary['experiment_duration_minutes'],\n",
    "        \"peak_memory_gb\": training_summary['peak_memory_usage'],\n",
    "        \"parameters_millions\": 11.7  # Same as baseline\n",
    "    }\n",
    "}\n",
    "\n",
    "all_experiments = {**previous_experiments, **current_experiment}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_experiments).T\n",
    "comparison_df.index.name = 'Experiment'\n",
    "\n",
    "print(\"📊 EXPERIMENT COMPARISON TABLE:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "comparison_df['accuracy_per_param'] = comparison_df['best_val_accuracy'] / comparison_df['parameters_millions']\n",
    "comparison_df['accuracy_per_minute'] = comparison_df['best_val_accuracy'] / comparison_df['training_time_minutes']\n",
    "comparison_df['accuracy_per_gb'] = comparison_df['best_val_accuracy'] / comparison_df['peak_memory_gb']\n",
    "\n",
    "print(f\"\\n⚡ EFFICIENCY METRICS:\")\n",
    "efficiency_df = comparison_df[['accuracy_per_param', 'accuracy_per_minute', 'accuracy_per_gb']]\n",
    "print(efficiency_df.round(4))\n",
    "\n",
    "# Find best performers\n",
    "best_accuracy = comparison_df['best_val_accuracy'].idxmax()\n",
    "best_efficiency = comparison_df['accuracy_per_param'].idxmax()\n",
    "fastest_training = comparison_df['training_time_minutes'].idxmin()\n",
    "most_memory_efficient = comparison_df['peak_memory_gb'].idxmin()\n",
    "\n",
    "print(f\"\\n🏆 PERFORMANCE LEADERS:\")\n",
    "print(f\"   Best Accuracy: {best_accuracy} ({comparison_df.loc[best_accuracy, 'best_val_accuracy']:.4f})\")\n",
    "print(f\"   Most Parameter Efficient: {best_efficiency} ({comparison_df.loc[best_efficiency, 'accuracy_per_param']:.4f})\")\n",
    "print(f\"   Fastest Training: {fastest_training} ({comparison_df.loc[fastest_training, 'training_time_minutes']:.1f} min)\")\n",
    "print(f\"   Most Memory Efficient: {most_memory_efficient} ({comparison_df.loc[most_memory_efficient, 'peak_memory_gb']:.1f} GB)\")\n",
    "\n",
    "# Current experiment analysis\n",
    "current_rank_accuracy = (comparison_df['best_val_accuracy'] < comparison_df.loc['resnet18_optimized', 'best_val_accuracy']).sum() + 1\n",
    "current_rank_efficiency = (comparison_df['accuracy_per_param'] < comparison_df.loc['resnet18_optimized', 'accuracy_per_param']).sum() + 1\n",
    "\n",
    "print(f\"\\n📈 CURRENT EXPERIMENT PERFORMANCE:\")\n",
    "print(f\"   Accuracy Rank: #{current_rank_accuracy} out of {len(comparison_df)}\")\n",
    "print(f\"   Parameter Efficiency Rank: #{current_rank_efficiency} out of {len(comparison_df)}\")\n",
    "\n",
    "# Log comparison results\n",
    "comparison_results = {\n",
    "    \"experiments_compared\": len(all_experiments),\n",
    "    \"current_accuracy_rank\": int(current_rank_accuracy),\n",
    "    \"current_efficiency_rank\": int(current_rank_efficiency),\n",
    "    \"best_overall_model\": best_accuracy,\n",
    "    \"most_efficient_model\": best_efficiency,\n",
    "    \"improvement_over_baseline\": float(comparison_df.loc['resnet18_optimized', 'best_val_accuracy'] - comparison_df.loc['resnet18_baseline', 'best_val_accuracy'])\n",
    "}\n",
    "\n",
    "logger.log_metrics(comparison_results, step=len(training_history), stage=\"experiment_comparison\")\n",
    "\n",
    "# Generate deployment recommendation\n",
    "print(f\"\\n🚀 DEPLOYMENT RECOMMENDATION:\")\n",
    "if current_rank_accuracy <= 2 and current_rank_efficiency <= 2:\n",
    "    recommendation = \"✅ RECOMMENDED FOR DEPLOYMENT - Excellent balance of accuracy and efficiency\"\nelif current_rank_accuracy <= 2:\n",
    "    recommendation = \"⚠️ CONSIDER FOR DEPLOYMENT - High accuracy but check efficiency constraints\"\nelif current_rank_efficiency <= 2:\n",
    "    recommendation = \"🤔 EFFICIENCY LEADER - Good for resource-constrained environments\"\nelse:\n",
    "    recommendation = \"❌ NOT RECOMMENDED - Better alternatives available\"\n\nprint(f\"   {recommendation}\")\nlogger.log_metadata({\"deployment_recommendation\": recommendation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Demo 7: Experiment Finalization and Cleanup\n",
    "\n",
    "Properly finalize the experiment with comprehensive logging and cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize experiment with comprehensive summary\n",
    "print(\"🏁 FINALIZING EXPERIMENT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Log final metadata\n",
    "final_metadata = {\n",
    "    \"experiment_status\": \"completed\",\n",
    "    \"end_time\": datetime.now().isoformat(),\n",
    "    \"total_logged_metrics\": len(training_history) * 8,  # Approximate\n",
    "    \"artifacts_created\": 3,  # dashboard, report, checkpoints\n",
    "    \"warnings_logged\": sum(1 for h in training_history if h['train_accuracy'] - h['val_accuracy'] > 0.1),\n",
    "    \"experiment_success\": all(success_criteria.values())\n",
    "}\n",
    "\n",
    "logger.log_metadata(final_metadata)\n",
    "\n",
    "# Generate experiment URL if available\n",
    "experiment_url = logger.get_experiment_url() if hasattr(logger, 'get_experiment_url') else None\n",
    "\n",
    "print(f\"✅ Experiment completed successfully!\")\n",
    "print(f\"📊 Total metrics logged: {final_metadata['total_logged_metrics']}\")\n",
    "print(f\"📁 Artifacts created: {final_metadata['artifacts_created']}\")\n",
    "print(f\"⚠️ Warnings logged: {final_metadata['warnings_logged']}\")\n",
    "if experiment_url:\n",
    "    print(f\"🔗 Experiment URL: {experiment_url}\")\n",
    "print(f\"📂 Local logs: {logger.log_dir}\")\n",
    "\n",
    "# End the experiment\n",
    "logger.end_experiment()\n",
    "\n",
    "print(f\"\\n📋 EXPERIMENT SUMMARY FOR PORTFOLIO:\")\n",
    "print(f\"   Project: {experiment_config.project_name}\")\n",
    "print(f\"   Best Result: {training_summary['best_validation_accuracy']:.1%} validation accuracy\")\n",
    "print(f\"   Training Efficiency: {training_summary['average_epoch_time']:.1f}s per epoch\")\n",
    "print(f\"   Resource Usage: {training_summary['peak_memory_usage']:.1f}GB peak memory\")\n",
    "print(f\"   Success Criteria: {sum(success_criteria.values())}/{len(success_criteria)} met\")\n",
    "print(f\"   Deployment Ready: {'Yes' if all(success_criteria.values()) else 'With modifications'}\")\n",
    "\n",
    "# Display file structure\n",
    "print(f\"\\n📁 GENERATED ARTIFACTS:\")\n",
    "log_dir = Path(\"./experiment_logs\")\n",
    "if log_dir.exists():\n",
    "    for file_path in sorted(log_dir.glob(\"**/*\")):\n",
    "        if file_path.is_file():\n",
    "            size_kb = file_path.stat().st_size / 1024\n",
    "            print(f\"   📄 {file_path.name} ({size_kb:.1f} KB)\")\n",
    "            \n",
    "print(f\"\\n🎉 Professional experiment logging demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Summary & Professional Applications\n",
    "\n",
    "This comprehensive demo showcased the experiment logging capabilities of the ML Cookbook:\n",
    "\n",
    "### ✅ Demonstrated Capabilities\n",
    "\n",
    "- **📝 Multi-Backend Logging** - Simultaneous logging to W&B, TensorBoard, and JSONL\n",
    "- **🏷️ Comprehensive Configuration Management** - Hyperparameters, metadata, and system info\n",
    "- **📊 Real-Time Metric Tracking** - Training progress, resource utilization, and custom metrics\n",
    "- **💾 Artifact Management** - Checkpoints, visualizations, and reports with versioning\n",
    "- **🔍 Experiment Comparison** - Multi-experiment analysis and ranking\n",
    "- **📋 Professional Reporting** - Stakeholder-ready summaries and recommendations\n",
    "\n",
    "### 🚀 Production Applications\n",
    "\n",
    "- **Research & Development** - Track experiment progress and compare model architectures\n",
    "- **Model Selection** - Data-driven model selection with comprehensive comparison metrics\n",
    "- **Performance Monitoring** - Track model performance degradation and resource usage\n",
    "- **Team Collaboration** - Shared experiment tracking and reproducible results\n",
    "- **Compliance & Auditing** - Complete audit trails for model development decisions\n",
    "\n",
    "### 💼 Portfolio Impact\n",
    "\n",
    "This demonstrates advanced ML engineering skills including:\n",
    "\n",
    "- **Systematic Experimentation** - Professional experiment design and tracking\n",
    "- **Data-Driven Decisions** - Evidence-based model selection and optimization\n",
    "- **Operational Excellence** - Production-ready monitoring and reporting systems\n",
    "- **Team Leadership** - Tools and processes that enable effective collaboration\n",
    "- **Business Communication** - Clear reporting of technical results to stakeholders\n",
    "\n",
    "### 🔗 Integration Benefits\n",
    "\n",
    "Experiment logging integrates seamlessly with:\n",
    "\n",
    "- **Performance Profiling** - Automatically log resource utilization metrics\n",
    "- **Statistical Validation** - Track statistical significance of model improvements\n",
    "- **Carbon Tracking** - Include sustainability metrics in experiment records\n",
    "- **CI/CD Pipelines** - Automated experiment logging in deployment workflows\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 Key Takeaway:** Professional experiment logging transforms ad-hoc ML development into systematic, reproducible, and auditable research processes that enable data-driven decision making and effective team collaboration.\n",
    "\n",
    "**Built with the ML Cookbook - Professional ML Engineering Toolkit** 🚀"
   ]
  }
 ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}