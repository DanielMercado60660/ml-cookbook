{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# 🔬 Performance Profiling Demo\\n\",\n    \"\\n\",\n    \"> **Professional ML performance analysis with the ML Cookbook measurement suite**\\n\",\n    \"\\n\",\n    \"This notebook demonstrates advanced performance profiling capabilities for ML workloads, including:\\n\",\n    \"\\n\",\n    \"- 📊 **Memory tracking** (peak RAM, step RAM, leak detection)\\n\",\n    \"- ⚡ **FLOPS estimation** for model operations\\n\",\n    \"- ⏱️ **Precision timing** with warm-up handling\\n\",\n    \"- 🖥️ **GPU utilization** monitoring\\n\",\n    \"- 📈 **Real-time visualization** of performance metrics\\n\",\n    \"\\n\",\n    \"**Use Case:** Optimize model training performance and identify bottlenecks before production deployment.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Import the ML Cookbook measurement suite\\n\",\n    \"from cookbook.measure import PerformanceProfiler\\n\",\n    \"\\n\",\n    \"# Standard ML libraries for realistic demo\\n\",\n    \"import torch\\n\",\n    \"import torch.nn as nn\\n\",\n    \"import numpy as np\\n\",\n    \"import time\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import pandas as pd\\n\",\n    \"\\n\",\n    \"# Configure plotting\\n\",\n    \"plt.style.use('seaborn-v0_8')\\n\",\n    \"plt.rcParams['figure.figsize'] = (12, 8)\\n\",\n    \"\\n\",\n    \"print(\\\"🔬 ML Cookbook Performance Profiling Demo\\\")\\n\",\n    \"print(\\\"=\\\" * 50)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 🎯 Demo 1: Basic Performance Profiling\\n\",\n    \"\\n\",\n    \"Let's start with a simple neural network training loop to demonstrate core profiling capabilities.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Create a simple neural network for demonstration\\n\",\n    \"class SimpleNet(nn.Module):\\n\",\n    \"    def __init__(self, input_size=784, hidden_size=512, output_size=10):\\n\",\n    \"        super().__init__()\\n\",\n    \"        self.layers = nn.Sequential(\\n\",\n    \"            nn.Linear(input_size, hidden_size),\\n\",\n    \"            nn.ReLU(),\\n\",\n    \"            nn.Linear(hidden_size, hidden_size),\\n\",\n    \"            nn.ReLU(),\\n\",\n    \"            nn.Linear(hidden_size, output_size)\\n\",\n    \"        )\\n\",\n    \"    \\n\",\n    \"    def forward(self, x):\\n\",\n    \"        return self.layers(x)\\n\",\n    \"\\n\",\n    \"# Initialize model and data\\n\",\n    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n    \"model = SimpleNet().to(device)\\n\",\n    \"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n\",\n    \"criterion = nn.CrossEntropyLoss()\\n\",\n    \"\\n\",\n    \"print(f\\\"Model device: {device}\\\")\\n\",\n    \"print(f\\\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Initialize the performance profiler\\n\",\n    \"profiler = PerformanceProfiler(\\n\",\n    \"    track_gpu=True,\\n\",\n    \"    track_carbon=False,  # Focus on performance for this demo\\n\",\n    \"    detailed_memory=True\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Profile the training loop\\n\",\n    \"print(\\\"🚀 Starting performance profiling...\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Performance profiling context manager\\n\",\n    \"with profiler.profile(\\\"training_loop_demo\\\") as prof:\\n\",\n    \"    \\n\",\n    \"    # Training loop simulation\\n\",\n    \"    num_batches = 20\\n\",\n    \"    batch_size = 128\\n\",\n    \"    \\n\",\n    \"    for batch_idx in range(num_batches):\\n\",\n    \"        # Generate synthetic batch\\n\",\n    \"        x = torch.randn(batch_size, 784).to(device)\\n\",\n    \"        y = torch.randint(0, 10, (batch_size,)).to(device)\\n\",\n    \"        \\n\",\n    \"        # Forward pass\\n\",\n    \"        outputs = model(x)\\n\",\n    \"        loss = criterion(outputs, y)\\n\",\n    \"        \\n\",\n    \"        # Backward pass\\n\",\n    \"        optimizer.zero_grad()\\n\",\n    \"        loss.backward()\\n\",\n    \"        optimizer.step()\\n\",\n    \"        \\n\",\n    \"        if batch_idx % 5 == 0:\\n\",\n    \"            print(f\\\"Batch {batch_idx:2d}/20 - Loss: {loss.item():.4f}\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"✅ Training loop completed!\\\")\\n\",\n    \"print(\\\"📊 Analyzing performance metrics...\\\")\"]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Get comprehensive performance metrics\\n\",\n    \"metrics = profiler.get_metrics()\\n\",\n    \"\\n\",\n    \"print(\\\"\\\" + \\\"=\\\" * 70)\\n\",\n    \"print(\\\"🔬 PERFORMANCE ANALYSIS RESULTS\\\")\\n\",\n    \"print(\\\"=\\\" * 70)\\n\",\n    \"\\n\",\n    \"# Memory analysis\\n\",\n    \"print(\\\"📊 Memory Usage:\\\")\\n\",\n    \"print(f\\\"   Peak RAM: {metrics.memory.peak_ram_mb:.1f} MB\\\")\\n\",\n    \"print(f\\\"   Final RAM: {metrics.memory.final_ram_mb:.1f} MB\\\")\\n\",\n    \"print(f\\\"   Peak GPU Memory: {metrics.memory.peak_gpu_mb:.1f} MB\\\" if hasattr(metrics.memory, 'peak_gpu_mb') else \\\"   GPU: Not available\\\")\\n\",\n    \"\\n\",\n    \"# Timing analysis\\n\",\n    \"print(\\\"⏱️ Timing Analysis:\\\")\\n\",\n    \"print(f\\\"   Total Wall Time: {metrics.timing.wall_time_s:.3f}s\\\")\\n\",\n    \"print(f\\\"   Average Batch Time: {metrics.timing.wall_time_s/num_batches:.3f}s\\\")\\n\",\n    \"\\n\",\n    \"# Compute analysis\\n\",\n    \"print(\\\"⚡ Compute Analysis:\\\")\\n\",\n    \"if hasattr(metrics.compute, 'estimated_flops') and metrics.compute.estimated_flops > 0:\\n\",\n    \"    print(f\\\"   Estimated FLOPs: {metrics.compute.estimated_flops:,}\\\")\\n\",\n    \"    print(f\\\"   FLOPs per second: {metrics.compute.estimated_flops/metrics.timing.wall_time_s:,.0f}\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"   FLOP estimation: Not available for this workload\\\")\\n\",\n    \"\\n\",\n    \"# Performance insights\\n\",\n    \"print(\\\"💡 Performance Insights:\\\")\\n\",\n    \"throughput = (num_batches * batch_size) / metrics.timing.wall_time_s\\n\",\n    \"print(f\\\"   Training throughput: {throughput:.0f} samples/second\\\")\\n\",\n    \"print(f\\\"   Memory efficiency: {metrics.memory.peak_ram_mb/1024:.2f} GB RAM for {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\\\")\"]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 📊 Demo 2: Comparative Performance Analysis\\n\",\n    \"\\n\",\n    \"Let's compare performance across different model configurations to demonstrate optimization insights.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Test different model configurations\\n\",\n    \"configs = [\\n\",\n    \"    {'hidden_size': 256, 'name': 'Small'},\\n\",\n    \"    {'hidden_size': 512, 'name': 'Medium'},\\n\",\n    \"    {'hidden_size': 1024, 'name': 'Large'}\\n\",\n    \"]\\n\",\n    \"\\n\",\n    \"results = []\\n\",\n    \"\\n\",\n    \"print(\\\"🧪 Running comparative performance analysis...\\\")\\n\",\n    \"print(\\\"=\\\" * 50)\\n\",\n    \"\\n\",\n    \"for config in configs:\\n\",\n    \"    print(f\\\"\\\\n🔬 Testing {config['name']} model (hidden_size={config['hidden_size']})...\\\")\\n\",\n    \"    \\n\",\n    \"    # Create model for this configuration\\n\",\n    \"    test_model = SimpleNet(hidden_size=config['hidden_size']).to(device)\\n\",\n    \"    test_optimizer = torch.optim.Adam(test_model.parameters(), lr=0.001)\\n\",\n    \"    \\n\",\n    \"    # Create fresh profiler for this test\\n\",\n    \"    test_profiler = PerformanceProfiler(track_gpu=True, track_carbon=False)\\n\",\n    \"    \\n\",\n    \"    # Profile training\\n\",\n    \"    with test_profiler.profile(f\\\"{config['name']}_model_training\\\") as prof:\\n\",\n    \"        for batch_idx in range(10):  # Shorter run for comparison\\n\",\n    \"            x = torch.randn(64, 784).to(device)  # Smaller batch for fair comparison\\n\",\n    \"            y = torch.randint(0, 10, (64,)).to(device)\\n\",\n    \"            \\n\",\n    \"            outputs = test_model(x)\\n\",\n    \"            loss = criterion(outputs, y)\\n\",\n    \"            \\n\",\n    \"            test_optimizer.zero_grad()\\n\",\n    \"            loss.backward()\\n\",\n    \"            test_optimizer.step()\\n\",\n    \"    \\n\",\n    \"    # Collect metrics\\n\",\n    \"    test_metrics = test_profiler.get_metrics()\\n\",\n    \"    param_count = sum(p.numel() for p in test_model.parameters())\\n\",\n    \"    \\n\",\n    \"    result = {\\n\",\n    \"        'Model': config['name'],\\n\",\n    \"        'Hidden Size': config['hidden_size'],\\n\",\n    \"        'Parameters (M)': param_count / 1e6,\\n\",\n    \"        'Peak RAM (MB)': test_metrics.memory.peak_ram_mb,\\n\",\n    \"        'Wall Time (s)': test_metrics.timing.wall_time_s,\\n\",\n    \"        'Throughput (samples/s)': (10 * 64) / test_metrics.timing.wall_time_s\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    if hasattr(test_metrics.memory, 'peak_gpu_mb'):\\n\",\n    \"        result['Peak GPU (MB)'] = test_metrics.memory.peak_gpu_mb\\n\",\n    \"    \\n\",\n    \"    results.append(result)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"   Parameters: {param_count/1e6:.1f}M\\\")\\n\",\n    \"    print(f\\\"   Peak RAM: {test_metrics.memory.peak_ram_mb:.1f} MB\\\")\\n\",\n    \"    print(f\\\"   Wall Time: {test_metrics.timing.wall_time_s:.3f}s\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"✅ Comparative analysis completed!\\\")\"]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Create comparison DataFrame\\n\",\n    \"comparison_df = pd.DataFrame(results)\\n\",\n    \"print(\\\"📊 Performance Comparison Results:\\\")\\n\",\n    \"print(\\\"=\\\" * 50)\\n\",\n    \"print(comparison_df.round(2))\"]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Create visualization of performance scaling\\n\",\n    \"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\\n\",\n    \"fig.suptitle('🔬 Performance Scaling Analysis', fontsize=16, fontweight='bold')\\n\",\n    \"\\n\",\n    \"# Plot 1: Memory usage vs model size\\n\",\n    \"ax1.plot(comparison_df['Parameters (M)'], comparison_df['Peak RAM (MB)'], 'o-', linewidth=2, markersize=8)\\n\",\n    \"ax1.set_xlabel('Parameters (Millions)')\\n\",\n    \"ax1.set_ylabel('Peak RAM (MB)')\\n\",\n    \"ax1.set_title('Memory Usage vs Model Size')\\n\",\n    \"ax1.grid(True, alpha=0.3)\\n\",\n    \"\\n\",\n    \"# Plot 2: Training throughput\\n\",\n    \"colors = ['green', 'orange', 'red']\\n\",\n    \"bars = ax2.bar(comparison_df['Model'], comparison_df['Throughput (samples/s)'], color=colors, alpha=0.7)\\n\",\n    \"ax2.set_ylabel('Throughput (samples/second)')\\n\",\n    \"ax2.set_title('Training Throughput by Model Size')\\n\",\n    \"ax2.grid(True, alpha=0.3, axis='y')\\n\",\n    \"\\n\",\n    \"# Add value labels on bars\\n\",\n    \"for bar, value in zip(bars, comparison_df['Throughput (samples/s)']):  \\n\",\n    \"    height = bar.get_height()\\n\",\n    \"    ax2.text(bar.get_x() + bar.get_width()/2., height + 10, f'{value:.0f}', \\n\",\n    \"             ha='center', va='bottom', fontweight='bold')\\n\",\n    \"\\n\",\n    \"# Plot 3: Memory efficiency (RAM per million parameters)\\n\",\n    \"memory_efficiency = comparison_df['Peak RAM (MB)'] / comparison_df['Parameters (M)']\\n\",\n    \"ax3.plot(comparison_df['Parameters (M)'], memory_efficiency, 's-', linewidth=2, markersize=8, color='purple')\\n\",\n    \"ax3.set_xlabel('Parameters (Millions)')\\n\",\n    \"ax3.set_ylabel('RAM per Million Parameters (MB)')\\n\",\n    \"ax3.set_title('Memory Efficiency')\\n\",\n    \"ax3.grid(True, alpha=0.3)\\n\",\n    \"\\n\",\n    \"# Plot 4: Training time scaling\\n\",\n    \"ax4.plot(comparison_df['Parameters (M)'], comparison_df['Wall Time (s)'], '^-', linewidth=2, markersize=8, color='darkred')\\n\",\n    \"ax4.set_xlabel('Parameters (Millions)')\\n\",\n    \"ax4.set_ylabel('Wall Time (seconds)')\\n\",\n    \"ax4.set_title('Training Time vs Model Complexity')\\n\",\n    \"ax4.grid(True, alpha=0.3)\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 🎯 Demo 3: Production Optimization Insights\\n\",\n    \"\\n\",\n    \"Let's demonstrate how to use profiling data to make production optimization decisions.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Analyze the results for production insights\\n\",\n    \"print(\\\"💡 PRODUCTION OPTIMIZATION INSIGHTS\\\")\\n\",\n    \"print(\\\"=\\\" * 50)\\n\",\n    \"\\n\",\n    \"# Find the most efficient model\\n\",\n    \"comparison_df['Efficiency Score'] = (comparison_df['Throughput (samples/s)'] / comparison_df['Peak RAM (MB)']) * 1000\\n\",\n    \"best_efficiency_idx = comparison_df['Efficiency Score'].idxmax()\\n\",\n    \"best_model = comparison_df.iloc[best_efficiency_idx]\\n\",\n    \"\\n\",\n    \"print(f\\\"🏆 Most Efficient Model: {best_model['Model']}\\\")\\n\",\n    \"print(f\\\"   Efficiency Score: {best_model['Efficiency Score']:.2f}\\\")\\n\",\n    \"print(f\\\"   Throughput: {best_model['Throughput (samples/s)']:.0f} samples/s\\\")\\n\",\n    \"print(f\\\"   Memory Usage: {best_model['Peak RAM (MB)']:.0f} MB\\\")\\n\",\n    \"\\n\",\n    \"# Memory scaling analysis\\n\",\n    \"memory_scaling_rate = (comparison_df['Peak RAM (MB)'].iloc[-1] - comparison_df['Peak RAM (MB)'].iloc[0]) / (comparison_df['Parameters (M)'].iloc[-1] - comparison_df['Parameters (M)'].iloc[0])\\n\",\n    \"print(f\\\"📊 Memory Scaling Rate: {memory_scaling_rate:.1f} MB per million parameters\\\")\\n\",\n    \"\\n\",\n    \"# Throughput vs complexity tradeoff\\n\",\n    \"throughput_drop = ((comparison_df['Throughput (samples/s)'].iloc[0] - comparison_df['Throughput (samples/s)'].iloc[-1]) / comparison_df['Throughput (samples/s)'].iloc[0]) * 100\\n\",\n    \"complexity_increase = ((comparison_df['Parameters (M)'].iloc[-1] - comparison_df['Parameters (M)'].iloc[0]) / comparison_df['Parameters (M)'].iloc[0]) * 100\\n\",\n    \"\\n\",\n    \"print(f\\\"⚖️ Performance Trade-offs:\\\")\\n\",\n    \"print(f\\\"   {complexity_increase:.0f}% more parameters → {throughput_drop:.0f}% throughput decrease\\\")\\n\",\n    \"\\n\",\n    \"# Production recommendations\\n\",\n    \"print(\\\"🚀 Production Recommendations:\\\")\\n\",\n    \"\\n\",\n    \"if throughput_drop < 50:\\n\",\n    \"    print(\\\"   ✅ Reasonable scaling - larger models viable for production\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"   ⚠️  Significant throughput drop - consider model compression\\\")\\n\",\n    \"\\n\",\n    \"if memory_scaling_rate < 100:\\n\",\n    \"    print(\\\"   ✅ Efficient memory usage - good for resource-constrained deployments\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"   ⚠️  High memory overhead - may need gradient checkpointing\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"   💡 Next steps: Profile on target production hardware\\\")\\n\",\n    \"print(\\\"   💡 Consider mixed precision training for 30-50% memory savings\\\")\\n\",\n    \"print(\\\"   💡 Benchmark with realistic batch sizes and sequence lengths\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 🔧 Demo 4: Advanced Profiling Features\\n\",\n    \"\\n\",\n    \"Explore advanced capabilities like memory leak detection and bottleneck identification.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Demonstrate memory leak detection\\n\",\n    \"print(\\\"🔍 Advanced Profiling: Memory Leak Detection\\\")\\n\",\n    \"print(\\\"=\\\" * 50)\\n\",\n    \"\\n\",\n    \"# Create a profiler with detailed memory tracking\\n\",\n    \"advanced_profiler = PerformanceProfiler(\\n\",\n    \"    track_gpu=True,\\n\",\n    \"    detailed_memory=True,\\n\",\n    \"    memory_interval=0.1  # Sample every 100ms\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Simulate a workload with potential memory issues\\n\",\n    \"memory_snapshots = []\\n\",\n    \"\\n\",\n    \"with advanced_profiler.profile(\\\"memory_analysis\\\") as prof:\\n\",\n    \"    model = SimpleNet(hidden_size=256).to(device)\\n\",\n    \"    optimizer = torch.optim.Adam(model.parameters())\\n\",\n    \"    \\n\",\n    \"    # Simulate training with potential memory accumulation\\n\",\n    \"    accumulated_tensors = []\\n\",\n    \"    \\n\",\n    \"    for epoch in range(5):\\n\",\n    \"        print(f\\\"   Epoch {epoch+1}/5 processing...\\\")\\n\",\n    \"        \\n\",\n    \"        for batch_idx in range(10):\\n\",\n    \"            x = torch.randn(32, 784).to(device)\\n\",\n    \"            y = torch.randint(0, 10, (32,)).to(device)\\n\",\n    \"            \\n\",\n    \"            outputs = model(x)\\n\",\n    \"            loss = criterion(outputs, y)\\n\",\n    \"            \\n\",\n    \"            optimizer.zero_grad()\\n\",\n    \"            loss.backward()\\n\",\n    \"            optimizer.step()\\n\",\n    \"            \\n\",\n    \"            # Intentionally accumulate some tensors (simulating a memory issue)\\n\",\n    \"            if batch_idx < 3:  # Only accumulate a few to avoid OOM\\n\",\n    \"                accumulated_tensors.append(x.clone().detach())\\n\",\n    \"        \\n\",\n    \"        # Clear some tensors mid-training (partial cleanup)\\n\",\n    \"        if epoch == 2:\\n\",\n    \"            accumulated_tensors = accumulated_tensors[:5]  # Keep some for demonstration\\n\",\n    \"\\n\",\n    \"print(\\\"✅ Memory analysis completed\\\")\\n\",\n    \"\\n\",\n    \"# Analyze memory patterns\\n\",\n    \"advanced_metrics = advanced_profiler.get_metrics()\\n\",\n    \"print(f\\\"📊 Memory Analysis Results:\\\")\\n\",\n    \"print(f\\\"   Peak RAM: {advanced_metrics.memory.peak_ram_mb:.1f} MB\\\")\\n\",\n    \"print(f\\\"   Final RAM: {advanced_metrics.memory.final_ram_mb:.1f} MB\\\")\\n\",\n    \"print(f\\\"   Memory Growth: {advanced_metrics.memory.final_ram_mb - advanced_metrics.memory.peak_ram_mb:.1f} MB\\\")\\n\",\n    \"\\n\",\n    \"if advanced_metrics.memory.final_ram_mb > advanced_metrics.memory.peak_ram_mb * 0.8:\\n\",\n    \"    print(\\\"   ⚠️  Potential memory accumulation detected\\\")\\n\",\n    \"    print(\\\"   💡 Recommendation: Check for tensor retention or gradient accumulation\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"   ✅ Healthy memory usage pattern\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 🎉 Summary & Next Steps\\n\",\n    \"\\n\",\n    \"This demo showcased the professional-grade performance profiling capabilities of the ML Cookbook measurement suite:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(\\\"🎯 DEMO SUMMARY & KEY TAKEAWAYS\\\")\\n\",\n    \"print(\\\"=\\\" * 50)\\n\",\n    \"\\n\",\n    \"print(\\\"✅ Demonstrated Capabilities:\\\")\\n\",\n    \"print(\\\"   🔬 Comprehensive performance profiling (memory, timing, compute)\\\")\\n\",\n    \"print(\\\"   📊 Comparative analysis across model configurations\\\")\\n\",\n    \"print(\\\"   💡 Production optimization insights and recommendations\\\")\\n\",\n    \"print(\\\"   🔍 Advanced memory analysis and leak detection\\\")\\n\",\n    \"print(\\\"   📈 Professional-grade visualizations and reporting\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"🚀 Production Applications:\\\")\\n\",\n    \"print(\\\"   • Model architecture optimization before large-scale training\\\")\\n\",\n    \"print(\\\"   • Resource planning for production deployments\\\")\\n\",\n    \"print(\\\"   • Performance regression detection in CI/CD pipelines\\\")\\n\",\n    \"print(\\\"   • Bottleneck identification in distributed training\\\")\\n\",\n    \"print(\\\"   • Memory optimization for resource-constrained environments\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"🔗 Next Steps:\\\")\\n\",\n    \"print(\\\"   1. Try the other ML Cookbook demos (carbon tracking, statistical validation)\\\")\\n\",\n    \"print(\\\"   2. Profile your own models and training loops\\\")\\n\",\n    \"print(\\\"   3. Integrate profiling into your MLOps pipelines\\\")\\n\",\n    \"print(\\\"   4. Explore advanced features like distributed training profiling\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"💼 Portfolio Impact:\\\")\\n\",\n    \"print(\\\"   This demonstrates advanced ML engineering skills including:\\\")\\n\",\n    \"print(\\\"   • Performance optimization expertise\\\")\\n\",\n    \"print(\\\"   • Production-ready tooling development\\\")\\n\",\n    \"print(\\\"   • Data-driven decision making for model architecture\\\")\\n\",\n    \"print(\\\"   • Professional software engineering practices\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"🌟 Built with the ML Cookbook - Professional ML Engineering Toolkit\\\")\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}\n