{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Statistical Validation Demo\n",
    "\n",
    "> **Rigorous A/B testing and significance analysis for ML model comparison**\n",
    "\n",
    "This notebook demonstrates professional statistical validation capabilities including:\n",
    "\n",
    "- üß™ **Multiple statistical tests** (t-test, Mann-Whitney U, bootstrap)\n",
    "- üéØ **Effect size calculations** with interpretations\n",
    "- üìä **Bootstrap confidence intervals** for robust estimation\n",
    "- üîÑ **Multiple comparison corrections** to avoid false discoveries\n",
    "- üìã **Experimental design recommendations** for optimal statistical power\n",
    "\n",
    "**Use Case:** Make statistically sound decisions when comparing model performance, ensuring that observed differences are genuine improvements rather than random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ML Cookbook statistical validation suite\n",
    "from cookbook.measure import (\n",
    "    StatisticalValidator,\n",
    "    TestType,\n",
    "    EffectSize,\n",
    "    StatisticalResult\n",
    ")\n",
    "\n",
    "# Standard libraries for demo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìà ML COOKBOOK - STATISTICAL VALIDATION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Demonstrating rigorous statistical analysis for ML model comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Demo 1: Basic Model Comparison\n",
    "\n",
    "Let's start with a fundamental scenario: comparing the performance of two different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize statistical validator\n",
    "validator = StatisticalValidator()\n",
    "\n",
    "# Simulate realistic model performance data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Baseline model (e.g., Random Forest) performance on 10 different test sets\n",
    "baseline_accuracies = np.random.normal(0.87, 0.03, 10)  # Mean=87%, std=3%\n",
    "baseline_accuracies = np.clip(baseline_accuracies, 0.8, 0.95)  # Realistic bounds\n",
    "\n",
    "# New model (e.g., Neural Network) performance on the same test sets\n",
    "new_model_accuracies = np.random.normal(0.91, 0.025, 10)  # Mean=91%, std=2.5%\n",
    "new_model_accuracies = np.clip(new_model_accuracies, 0.85, 0.96)  # Realistic bounds\n",
    "\n",
    "print(\"üß™ BASIC MODEL COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Baseline Model (Random Forest):\")\n",
    "print(f\"   Accuracies: {[f'{acc:.3f}' for acc in baseline_accuracies]}\")\n",
    "print(f\"   Mean: {baseline_accuracies.mean():.3f} ¬± {baseline_accuracies.std():.3f}\")\n",
    "\n",
    "print(f\"\\nNew Model (Neural Network):\")\n",
    "print(f\"   Accuracies: {[f'{acc:.3f}' for acc in new_model_accuracies]}\")\n",
    "print(f\"   Mean: {new_model_accuracies.mean():.3f} ¬± {new_model_accuracies.std():.3f}\")\n",
    "\n",
    "print(f\"\\nObserved Improvement: {(new_model_accuracies.mean() - baseline_accuracies.mean()) * 100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical comparison using Welch's t-test\n",
    "comparison_result = validator.compare_models(\n",
    "    baseline_scores=baseline_accuracies.tolist(),\n",
    "    variant_scores=new_model_accuracies.tolist(),\n",
    "    test_type=TestType.WELCH_T_TEST,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"üìä STATISTICAL TEST RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Test Type: Welch's t-test\")\n",
    "print(f\"Null Hypothesis: No difference between models\")\n",
    "print(f\"Alternative: Neural Network performs differently\")\n",
    "print()\n",
    "print(f\"üìà Results:\")\n",
    "print(f\"   p-value: {comparison_result.p_value:.4f}\")\n",
    "print(f\"   Significant difference: {'‚úÖ YES' if comparison_result.is_significant else '‚ùå NO'}\")\n",
    "print(f\"   Effect size (Cohen's d): {comparison_result.effect_size.magnitude:.3f}\")\n",
    "print(f\"   Effect interpretation: {comparison_result.effect_size.interpretation}\")\n",
    "print(f\"   Confidence interval: [{comparison_result.confidence_interval[0]:.3f}, {comparison_result.confidence_interval[1]:.3f}]\")\n",
    "\n",
    "# Interpretation\n",
    "if comparison_result.is_significant:\n",
    "    print(f\"\\nüéØ CONCLUSION:\")\n",
    "    print(f\"   The Neural Network shows a statistically significant improvement\")\n",
    "    print(f\"   over the Random Forest baseline (p < 0.05).\")\n",
    "    \n",
    "    if comparison_result.effect_size.magnitude > 0.8:\n",
    "        print(f\"   The effect size indicates a LARGE practical significance.\")\n",
    "        print(f\"   Recommendation: ‚úÖ Deploy the Neural Network\")\n",
    "    elif comparison_result.effect_size.magnitude > 0.5:\n",
    "        print(f\"   The effect size indicates a MEDIUM practical significance.\")\n",
    "        print(f\"   Recommendation: ‚ö†Ô∏è Consider deployment costs vs. benefits\")\n",
    "    else:\n",
    "        print(f\"   The effect size indicates a SMALL practical significance.\")\n",
    "        print(f\"   Recommendation: ü§î May not justify deployment costs\")\nelse:\n",
    "    print(f\"\\nüéØ CONCLUSION:\")\n",
    "    print(f\"   No statistically significant difference found.\")\n",
    "    print(f\"   Recommendation: ‚ùå Insufficient evidence to change models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Demo 2: Visualizing Statistical Comparisons\n",
    "\n",
    "Professional visualizations make statistical results accessible to stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('üìà Statistical Model Comparison Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Box plots comparing distributions\n",
    "ax1.boxplot([baseline_accuracies, new_model_accuracies], \n",
    "           labels=['Random Forest\\n(Baseline)', 'Neural Network\\n(New Model)'],\n",
    "           patch_artist=True,\n",
    "           boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "           medianprops=dict(color='red', linewidth=2))\n",
    "ax1.set_title('üéØ Model Performance Distributions', fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add significance indicator\n",
    "if comparison_result.is_significant:\n",
    "    ax1.text(0.5, 0.95, '***', transform=ax1.transAxes, ha='center', \n",
    "             fontsize=20, fontweight='bold', color='red')\n",
    "    ax1.text(0.5, 0.90, 'p < 0.05', transform=ax1.transAxes, ha='center', \n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Individual data points with trend\n",
    "x_baseline = np.ones(len(baseline_accuracies)) + np.random.normal(0, 0.05, len(baseline_accuracies))\n",
    "x_new = np.ones(len(new_model_accuracies)) * 2 + np.random.normal(0, 0.05, len(new_model_accuracies))\n",
    "\n",
    "ax2.scatter(x_baseline, baseline_accuracies, alpha=0.7, s=100, color='lightcoral', label='Random Forest')\n",
    "ax2.scatter(x_new, new_model_accuracies, alpha=0.7, s=100, color='lightgreen', label='Neural Network')\n",
    "\n",
    "# Add means with confidence intervals\n",
    "baseline_mean = baseline_accuracies.mean()\n",
    "baseline_ci = stats.sem(baseline_accuracies) * stats.t.ppf(0.975, len(baseline_accuracies)-1)\n",
    "new_mean = new_model_accuracies.mean()\n",
    "new_ci = stats.sem(new_model_accuracies) * stats.t.ppf(0.975, len(new_model_accuracies)-1)\n",
    "\n",
    "ax2.errorbar(1, baseline_mean, yerr=baseline_ci, fmt='o', color='red', markersize=10, linewidth=3, label='Mean ¬± 95% CI')\n",
    "ax2.errorbar(2, new_mean, yerr=new_ci, fmt='o', color='darkgreen', markersize=10, linewidth=3)\n",
    "\n",
    "ax2.set_xlim(0.5, 2.5)\n",
    "ax2.set_xticks([1, 2])\n",
    "ax2.set_xticklabels(['Random Forest', 'Neural Network'])\n",
    "ax2.set_title('üìä Individual Results with Confidence Intervals', fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Effect size visualization\n",
    "effect_sizes = ['Negligible\\n(< 0.2)', 'Small\\n(0.2-0.5)', 'Medium\\n(0.5-0.8)', 'Large\\n(> 0.8)']\n",
    "effect_thresholds = [0.2, 0.5, 0.8, 2.0]\n",
    "colors = ['lightgray', 'yellow', 'orange', 'red']\n",
    "\n",
    "# Color bars based on observed effect size\n",
    "observed_effect = abs(comparison_result.effect_size.magnitude)\n",
    "bar_colors = []\n",
    "for i, threshold in enumerate(effect_thresholds):\n",
    "    if observed_effect < threshold:\n",
    "        bar_colors = ['darkgreen' if j == i else colors[j] for j in range(len(colors))]\n",
    "        break\n",
    "\n",
    "bars = ax3.bar(effect_sizes, effect_thresholds, color=bar_colors, alpha=0.7)\n",
    "ax3.axhline(y=observed_effect, color='black', linestyle='--', linewidth=2, \n",
    "           label=f'Observed Effect Size: {observed_effect:.3f}')\n",
    "ax3.set_title('üìè Effect Size Interpretation', fontweight='bold')\n",
    "ax3.set_ylabel('Cohen\\'s d')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Statistical power analysis\n",
    "sample_sizes = range(5, 31, 2)\n",
    "powers = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    # Calculate statistical power for different sample sizes\n",
    "    # Using approximation for t-test power\n",
    "    effect_size = observed_effect\n",
    "    alpha = 0.05\n",
    "    df = 2 * n - 2\n",
    "    t_crit = stats.t.ppf(1 - alpha/2, df)\n",
    "    ncp = effect_size * np.sqrt(n / 2)  # Non-centrality parameter\n",
    "    power = 1 - stats.nct.cdf(t_crit, df, ncp) + stats.nct.cdf(-t_crit, df, ncp)\n",
    "    powers.append(power)\n",
    "\n",
    "ax4.plot(sample_sizes, powers, 'o-', linewidth=2, markersize=6, color='purple')\n",
    "ax4.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Desired Power (0.8)')\n",
    "ax4.axvline(x=len(baseline_accuracies), color='green', linestyle='-', alpha=0.7, \n",
    "           label=f'Current Sample Size ({len(baseline_accuracies)})')\n",
    "ax4.set_title('‚ö° Statistical Power Analysis', fontweight='bold')\n",
    "ax4.set_xlabel('Sample Size per Group')\n",
    "ax4.set_ylabel('Statistical Power')\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical recommendations\n",
    "current_power = powers[sample_sizes.index(len(baseline_accuracies))] if len(baseline_accuracies) in sample_sizes else 0.5\n",
    "print(f\"\\n‚ö° STATISTICAL POWER ANALYSIS:\")\n",
    "print(f\"   Current power with {len(baseline_accuracies)} samples per group: {current_power:.3f}\")\n",
    "if current_power < 0.8:\n",
    "    recommended_n = next((n for n, p in zip(sample_sizes, powers) if p >= 0.8), 30)\n",
    "    print(f\"   Recommendation: Increase sample size to {recommended_n} per group for 80% power\")\nelse:\n",
    "    print(f\"   ‚úÖ Current sample size provides adequate statistical power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Demo 3: Multiple Model Comparison with Correction\n",
    "\n",
    "When comparing multiple models simultaneously, we need to correct for multiple testing to avoid false discoveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate performance data for multiple models\n",
    "np.random.seed(123)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": np.random.normal(0.82, 0.04, 12),\n",
    "    \"Random Forest\": np.random.normal(0.87, 0.03, 12),\n",
    "    \"XGBoost\": np.random.normal(0.89, 0.025, 12),\n",
    "    \"Neural Network\": np.random.normal(0.91, 0.03, 12),\n",
    "    \"Ensemble\": np.random.normal(0.93, 0.02, 12)\n",
    "}\n",
    "\n",
    "# Clip to realistic bounds\n",
    "for model_name, scores in models.items():\n",
    "    models[model_name] = np.clip(scores, 0.7, 0.98)\n",
    "\n",
    "print(\"üß™ MULTIPLE MODEL COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display model summary statistics\n",
    "model_stats = []\n",
    "for model_name, scores in models.items():\n",
    "    stats_dict = {\n",
    "        \"Model\": model_name,\n",
    "        \"Mean\": scores.mean(),\n",
    "        \"Std\": scores.std(),\n",
    "        \"Min\": scores.min(),\n",
    "        \"Max\": scores.max()\n",
    "    }\n",
    "    model_stats.append(stats_dict)\n",
    "\n",
    "model_df = pd.DataFrame(model_stats)\n",
    "print(model_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pairwise comparisons between all models\n",
    "model_names = list(models.keys())\n",
    "comparison_results = []\n",
    "\n",
    "print(\"\\nüìà PAIRWISE STATISTICAL COMPARISONS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i + 1, len(model_names)):\n",
    "        model1, model2 = model_names[i], model_names[j]\n",
    "        scores1, scores2 = models[model1], models[model2]\n",
    "        \n",
    "        # Perform statistical comparison\n",
    "        result = validator.compare_models(\n",
    "            baseline_scores=scores1.tolist(),\n",
    "            variant_scores=scores2.tolist(),\n",
    "            test_type=TestType.WELCH_T_TEST,\n",
    "            alpha=0.05\n",
    "        )\n",
    "        \n",
    "        comparison_results.append({\n",
    "            \"Model 1\": model1,\n",
    "            \"Model 2\": model2,\n",
    "            \"Mean Diff\": scores2.mean() - scores1.mean(),\n",
    "            \"p-value\": result.p_value,\n",
    "            \"Significant\": result.is_significant,\n",
    "            \"Effect Size\": result.effect_size.magnitude,\n",
    "            \"Effect Interp\": result.effect_size.interpretation\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{model1} vs {model2}:\")\n",
    "        print(f\"   Mean difference: {scores2.mean() - scores1.mean():.4f}\")\n",
    "        print(f\"   p-value: {result.p_value:.4f}\")\n",
    "        print(f\"   Significant: {'‚úÖ' if result.is_significant else '‚ùå'}\")\n",
    "        print(f\"   Effect size: {result.effect_size.magnitude:.3f} ({result.effect_size.interpretation})\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(f\"\\nüìä COMPARISON SUMMARY:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multiple testing correction\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "p_values = comparison_df['p-value'].values\n",
    "rejected, p_corrected, alpha_sidak, alpha_bonf = multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "\n",
    "# Add corrected results to DataFrame\n",
    "comparison_df['p_corrected'] = p_corrected\n",
    "comparison_df['Significant_Corrected'] = rejected\n",
    "\n",
    "print(\"\\nüî¨ MULTIPLE TESTING CORRECTION (Bonferroni):\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Original alpha level: 0.05\")\n",
    "print(f\"Number of comparisons: {len(p_values)}\")\n",
    "print(f\"Corrected alpha level: {0.05/len(p_values):.4f}\")\n",
    "\n",
    "print(f\"\\nüìà RESULTS AFTER CORRECTION:\")\n",
    "corrected_summary = comparison_df[['Model 1', 'Model 2', 'p-value', 'p_corrected', 'Significant', 'Significant_Corrected']].copy()\n",
    "print(corrected_summary.round(4))\n",
    "\n",
    "# Count significant results\n",
    "sig_before = comparison_df['Significant'].sum()\n",
    "sig_after = comparison_df['Significant_Corrected'].sum()\n",
    "\n",
    "print(f\"\\nüéØ CORRECTION IMPACT:\")\n",
    "print(f\"   Significant results before correction: {sig_before}/{len(p_values)}\")\n",
    "print(f\"   Significant results after correction: {sig_after}/{len(p_values)}\")\n",
    "\n",
    "if sig_after < sig_before:\n",
    "    print(f\"   ‚ö†Ô∏è Multiple testing correction reduced false discoveries\")\n",
    "    print(f\"   Recommendation: Use corrected p-values for final decisions\")\nelse:\n",
    "    print(f\"   ‚úÖ Strong effects survived multiple testing correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Demo 4: Bootstrap Confidence Intervals\n",
    "\n",
    "Bootstrap methods provide robust confidence intervals without assuming normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bootstrap confidence intervals\n",
    "best_model_scores = models[\"Ensemble\"]\n",
    "second_best_scores = models[\"Neural Network\"]\n",
    "\n",
    "# Perform bootstrap comparison\n",
    "bootstrap_result = validator.compare_models(\n",
    "    baseline_scores=second_best_scores.tolist(),\n",
    "    variant_scores=best_model_scores.tolist(),\n",
    "    test_type=TestType.BOOTSTRAP,\n",
    "    alpha=0.05,\n",
    "    bootstrap_iterations=10000  # High number for accurate CI\n",
    ")\n",
    "\n",
    "print(\"üîÑ BOOTSTRAP STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Comparing: Neural Network vs Ensemble\")\n",
    "print(f\"Bootstrap iterations: 10,000\")\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Mean difference: {best_model_scores.mean() - second_best_scores.mean():.4f}\")\n",
    "print(f\"   Bootstrap p-value: {bootstrap_result.p_value:.4f}\")\n",
    "print(f\"   95% Confidence interval: [{bootstrap_result.confidence_interval[0]:.4f}, {bootstrap_result.confidence_interval[1]:.4f}]\")\n",
    "print(f\"   Significant improvement: {'‚úÖ YES' if bootstrap_result.is_significant else '‚ùå NO'}\")\n",
    "\n",
    "# Visualize bootstrap distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Original data comparison\n",
    "ax1.hist(second_best_scores, alpha=0.7, bins=8, label='Neural Network', color='lightblue', density=True)\n",
    "ax1.hist(best_model_scores, alpha=0.7, bins=8, label='Ensemble', color='lightgreen', density=True)\n",
    "ax1.axvline(second_best_scores.mean(), color='blue', linestyle='--', linewidth=2, label='NN Mean')\n",
    "ax1.axvline(best_model_scores.mean(), color='green', linestyle='--', linewidth=2, label='Ensemble Mean')\n",
    "ax1.set_title('üìä Original Score Distributions', fontweight='bold')\n",
    "ax1.set_xlabel('Accuracy')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bootstrap sampling distribution (simulated for visualization)\n",
    "np.random.seed(42)\n",
    "bootstrap_diffs = []\n",
    "for _ in range(1000):  # Simulate bootstrap for visualization\n",
    "    boot_nn = np.random.choice(second_best_scores, size=len(second_best_scores), replace=True)\n",
    "    boot_ensemble = np.random.choice(best_model_scores, size=len(best_model_scores), replace=True)\n",
    "    bootstrap_diffs.append(boot_ensemble.mean() - boot_nn.mean())\n",
    "\n",
    "ax2.hist(bootstrap_diffs, bins=30, alpha=0.7, color='purple', density=True)\n",
    "ax2.axvline(np.mean(bootstrap_diffs), color='red', linestyle='--', linewidth=2, label='Bootstrap Mean')\n",
    "ax2.axvline(bootstrap_result.confidence_interval[0], color='orange', linestyle=':', linewidth=2, label='95% CI')\n",
    "ax2.axvline(bootstrap_result.confidence_interval[1], color='orange', linestyle=':', linewidth=2)\n",
    "ax2.axvline(0, color='black', linestyle='-', alpha=0.5, label='No Difference')\n",
    "ax2.set_title('üîÑ Bootstrap Sampling Distribution', fontweight='bold')\n",
    "ax2.set_xlabel('Difference in Mean Accuracy')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "ci_lower, ci_upper = bootstrap_result.confidence_interval\n",
    "print(f\"\\nüéØ BOOTSTRAP INTERPRETATION:\")\n",
    "if ci_lower > 0:\n",
    "    print(f\"   ‚úÖ The entire 95% CI is above zero\")\n",
    "    print(f\"   Strong evidence that Ensemble > Neural Network\")\n",
    "    print(f\"   Minimum expected improvement: {ci_lower:.4f}\")\nelif ci_upper < 0:\n",
    "    print(f\"   ‚ùå The entire 95% CI is below zero\")\n",
    "    print(f\"   Strong evidence that Neural Network > Ensemble\")\nelse:\n",
    "    print(f\"   ‚ö†Ô∏è The 95% CI includes zero\")\n",
    "    print(f\"   Cannot conclusively determine which model is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Demo 5: Practical Significance vs Statistical Significance\n",
    "\n",
    "Statistical significance doesn't always mean practical significance. Let's explore this crucial distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate scenarios with different effect sizes\n",
    "scenarios = {\n",
    "    \"Large Effect, Small N\": {\n",
    "        \"baseline\": np.random.normal(0.80, 0.05, 8),\n",
    "        \"variant\": np.random.normal(0.90, 0.05, 8),  # Large effect\n",
    "        \"context\": \"Small dataset, large improvement\"\n",
    "    },\n",
    "    \"Small Effect, Large N\": {\n",
    "        \"baseline\": np.random.normal(0.85, 0.02, 50),\n",
    "        \"variant\": np.random.normal(0.86, 0.02, 50),  # Small effect\n",
    "        \"context\": \"Large dataset, small improvement\"\n",
    "    },\n",
    "    \"Medium Effect, Medium N\": {\n",
    "        \"baseline\": np.random.normal(0.82, 0.04, 20),\n",
    "        \"variant\": np.random.normal(0.88, 0.04, 20),  # Medium effect\n",
    "        \"context\": \"Balanced scenario\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚öñÔ∏è PRACTICAL vs STATISTICAL SIGNIFICANCE\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "scenario_results = []\n",
    "\n",
    "for scenario_name, scenario_data in scenarios.items():\n",
    "    baseline = scenario_data[\"baseline\"]\n",
    "    variant = scenario_data[\"variant\"]\n",
    "    \n",
    "    # Statistical test\n",
    "    result = validator.compare_models(\n",
    "        baseline_scores=baseline.tolist(),\n",
    "        variant_scores=variant.tolist(),\n",
    "        test_type=TestType.WELCH_T_TEST,\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    mean_diff = variant.mean() - baseline.mean()\n",
    "    \n",
    "    scenario_results.append({\n",
    "        \"Scenario\": scenario_name,\n",
    "        \"Sample Size\": len(baseline),\n",
    "        \"Mean Difference\": mean_diff,\n",
    "        \"Effect Size\": result.effect_size.magnitude,\n",
    "        \"p-value\": result.p_value,\n",
    "        \"Statistically Significant\": result.is_significant,\n",
    "        \"Effect Interpretation\": result.effect_size.interpretation,\n",
    "        \"Context\": scenario_data[\"context\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä {scenario_name}:\")\n",
    "    print(f\"   {scenario_data['context']}\")\n",
    "    print(f\"   Sample size: {len(baseline)} per group\")\n",
    "    print(f\"   Mean difference: {mean_diff:.4f}\")\n",
    "    print(f\"   Effect size: {result.effect_size.magnitude:.3f} ({result.effect_size.interpretation})\")\n",
    "    print(f\"   p-value: {result.p_value:.4f}\")\n",
    "    print(f\"   Statistically significant: {'‚úÖ' if result.is_significant else '‚ùå'}\")\n",
    "    \n",
    "    # Practical significance assessment\n",
    "    if abs(mean_diff) >= 0.05:  # 5 percentage points\n",
    "        practical_sig = \"High\"\n",
    "    elif abs(mean_diff) >= 0.02:  # 2 percentage points\n",
    "        practical_sig = \"Medium\"\n",
    "    else:\n",
    "        practical_sig = \"Low\"\n",
    "    \n",
    "    print(f\"   Practical significance: {practical_sig}\")\n",
    "    \n",
    "    # Decision recommendation\n",
    "    if result.is_significant and practical_sig in [\"High\", \"Medium\"]:\n",
    "        decision = \"‚úÖ DEPLOY - Both statistically and practically significant\"\n",
    "    elif result.is_significant and practical_sig == \"Low\":\n",
    "        decision = \"‚ö†Ô∏è CONSIDER - Statistically significant but small practical impact\"\n",
    "    elif not result.is_significant and practical_sig == \"High\":\n",
    "        decision = \"ü§î INVESTIGATE - Large effect but not statistically significant (power issue?)\"\n",
    "    else:\n",
    "        decision = \"‚ùå DO NOT DEPLOY - Insufficient evidence\"\n",
    "    \n",
    "    print(f\"   Decision: {decision}\")\n",
    "\n",
    "# Summary table\n",
    "results_df = pd.DataFrame(scenario_results)\n",
    "print(f\"\\nüìã SCENARIO COMPARISON SUMMARY:\")\n",
    "print(results_df[['Scenario', 'Sample Size', 'Mean Difference', 'Effect Size', 'p-value', 'Statistically Significant']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Demo 6: Experimental Design Recommendations\n",
    "\n",
    "The statistical validator can provide recommendations for optimal experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate experimental design guidance\n",
    "print(\"üî¨ EXPERIMENTAL DESIGN RECOMMENDATIONS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Scenario: Planning a new model comparison experiment\n",
    "expected_baseline_performance = 0.85\n",
    "minimum_detectable_effect = 0.03  # Want to detect 3 percentage point improvement\n",
    "desired_power = 0.80\n",
    "alpha = 0.05\n",
    "\n",
    "print(f\"üéØ EXPERIMENTAL PLANNING SCENARIO:\")\n",
    "print(f\"   Expected baseline performance: {expected_baseline_performance:.2%}\")\n",
    "print(f\"   Minimum effect size to detect: {minimum_detectable_effect:.1%} improvement\")\n",
    "print(f\"   Desired statistical power: {desired_power:.0%}\")\n",
    "print(f\"   Significance level: {alpha:.2%}\")\n",
    "\n",
    "# Calculate required sample size (simplified power analysis)\n",
    "# This is a simplified calculation - in practice, use specialized power analysis tools\n",
    "assumed_std = 0.04  # Assume 4% standard deviation based on historical data\n",
    "effect_size_cohen_d = minimum_detectable_effect / assumed_std\n",
    "\n",
    "print(f\"\\nüìä POWER ANALYSIS:\")\n",
    "print(f\"   Assumed standard deviation: {assumed_std:.1%}\")\n",
    "print(f\"   Cohen's d effect size: {effect_size_cohen_d:.2f}\")\n",
    "\n",
    "# Approximate sample size calculation for t-test\n",
    "# Using simplified formula: n ‚âà 2 * (z_Œ±/2 + z_Œ≤)¬≤ / Œ¥¬≤\n",
    "# where Œ¥ is the standardized effect size\n",
    "from scipy.stats import norm\n",
    "z_alpha = norm.ppf(1 - alpha/2)  # Two-tailed\n",
    "z_beta = norm.ppf(desired_power)  # Power\n",
    "n_per_group = int(np.ceil(2 * (z_alpha + z_beta)**2 / effect_size_cohen_d**2))\n",
    "\n",
    "print(f\"\\nüìà SAMPLE SIZE RECOMMENDATION:\")\n",
    "print(f\"   Required sample size per group: {n_per_group}\")\n",
    "print(f\"   Total samples needed: {n_per_group * 2}\")\n",
    "\n",
    "# Provide experimental design recommendations\n",
    "print(f\"\\nüìã EXPERIMENTAL DESIGN CHECKLIST:\")\n",
    "\n",
    "recommendations = [\n",
    "    f\"‚úÖ Collect {n_per_group} independent test samples per model\",\n",
    "    \"‚úÖ Use stratified sampling to ensure balanced class representation\",\n",
    "    \"‚úÖ Randomize the order of test samples to avoid temporal biases\",\n",
    "    \"‚úÖ Use the same test sets for both models (paired comparison)\",\n",
    "    \"‚úÖ Pre-register your analysis plan to avoid p-hacking\",\n",
    "    \"‚úÖ Plan for multiple testing correction if comparing >2 models\",\n",
    "    \"‚úÖ Define practical significance thresholds before data collection\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"   {rec}\")\n",
    "\n",
    "# Cost-benefit analysis\n",
    "print(f\"\\nüí∞ COST-BENEFIT CONSIDERATIONS:\")\n",
    "cost_per_sample = 10  # Assume $10 per test sample\n",
    "total_cost = n_per_group * 2 * cost_per_sample\n",
    "\n",
    "print(f\"   Estimated data collection cost: ${total_cost:,}\")\n",
    "print(f\"   Cost per percentage point detected: ${total_cost / (minimum_detectable_effect * 100):.0f}\")\n",
    "\n",
    "if minimum_detectable_effect >= 0.05:\n",
    "    print(f\"   üí° Detecting large effects (‚â•5pp) - cost-effective approach\")\nelif minimum_detectable_effect >= 0.02:\n",
    "    print(f\"   ‚ö†Ô∏è Detecting medium effects (2-5pp) - moderate cost investment\")\nelse:\n",
    "    print(f\"   üîç Detecting small effects (<2pp) - high cost, ensure business value\")\n",
    "\n",
    "print(f\"\\nüéØ FINAL RECOMMENDATIONS:\")\n",
    "print(f\"   1. Design experiment with {n_per_group} samples per group\")\n",
    "print(f\"   2. Use paired t-test for maximum statistical power\")\n",
    "print(f\"   3. Report both statistical and practical significance\")\n",
    "print(f\"   4. Consider cost vs. business value of detected improvements\")\n",
    "print(f\"   5. Use confidence intervals for more informative results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Summary & Professional Applications\n",
    "\n",
    "This comprehensive demo showcased the statistical validation capabilities of the ML Cookbook:\n",
    "\n",
    "### ‚úÖ Demonstrated Capabilities\n",
    "\n",
    "- **üß™ Multiple Statistical Tests** - t-tests, Mann-Whitney U, bootstrap methods\n",
    "- **üìä Effect Size Analysis** - Cohen's d with practical interpretations\n",
    "- **üîÑ Bootstrap Methods** - Robust confidence intervals without distributional assumptions\n",
    "- **‚öñÔ∏è Multiple Testing Correction** - Bonferroni and other methods to control false discovery rate\n",
    "- **üìà Power Analysis** - Sample size planning and experimental design guidance\n",
    "- **üéØ Practical vs Statistical Significance** - Business-relevant decision frameworks\n",
    "\n",
    "### üöÄ Production Applications\n",
    "\n",
    "- **A/B Testing for ML Models** - Rigorous model comparison in production environments\n",
    "- **Hyperparameter Optimization** - Statistical validation of parameter choices\n",
    "- **Feature Selection** - Quantifying the impact of different feature sets\n",
    "- **Model Monitoring** - Detecting statistically significant performance degradation\n",
    "- **Experimental Planning** - Optimal sample size determination for cost-effective testing\n",
    "\n",
    "### üíº Portfolio Impact\n",
    "\n",
    "This demonstrates advanced statistical competency including:\n",
    "\n",
    "- **Rigorous Methodology** - Proper experimental design and hypothesis testing\n",
    "- **Business Acumen** - Balancing statistical and practical significance\n",
    "- **Risk Management** - Multiple testing corrections and Type I/II error control\n",
    "- **Communication Skills** - Clear visualizations and stakeholder-friendly interpretations\n",
    "- **Professional Standards** - Following established statistical best practices\n",
    "\n",
    "### üîó Integration with ML Pipeline\n",
    "\n",
    "Statistical validation integrates seamlessly with:\n",
    "\n",
    "- **Performance Profiling** - Validate that performance improvements are statistically significant\n",
    "- **Experiment Logging** - Automatically track statistical test results and effect sizes\n",
    "- **Carbon Tracking** - Compare model efficiency improvements with proper statistical rigor\n",
    "- **Production Monitoring** - Continuous statistical monitoring of model performance\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Key Takeaway:** Statistical validation transforms subjective model comparisons into objective, evidence-based decisions with quantified confidence levels and practical significance assessments.\n",
    "\n",
    "**Built with the ML Cookbook - Professional ML Engineering Toolkit** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}